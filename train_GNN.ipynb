{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "%autoreload 2\n",
    "\n",
    "import recommender_utils\n",
    "import numpy as np\n",
    "import scipy.sparse as sp\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train item features shape: (8500, 20)\n",
      "Validation item features shape: (5536, 20)\n",
      "Test item features shape: (3975, 20)\n"
     ]
    }
   ],
   "source": [
    "movie_df, movie_feature_headers, num_feature_headers = recommender_utils.get_movies_data(filepath='data_small/movies.csv', separator=r',', movies_columns_to_drop=['genres'], only_genres=False)\n",
    "\n",
    "data_train = recommender_utils.get_ratings_data(filepath='data_small/train.csv', separator=r',', dtypes=recommender_utils.dtypes)\n",
    "data_val = recommender_utils.get_ratings_data(filepath='data_small/validate.csv', separator=r',', dtypes=recommender_utils.dtypes)\n",
    "data_test = recommender_utils.get_ratings_data(filepath='data_small/test.csv', separator=r',', dtypes=recommender_utils.dtypes)\n",
    "\n",
    "data_array_train = np.array(data_train.values.tolist())\n",
    "data_array_val = np.array(data_val.values.tolist())\n",
    "data_array_test = np.array(data_test.values.tolist())\n",
    "\n",
    "# add features for whole dataset TODO\n",
    "train_adjacency_mx, train_labels, train_user_idx, train_item_idx, train_item_dict = recommender_utils.preprocess_data_to_graph(data_array_train, dtypes=recommender_utils.dtypes, class_values=recommender_utils.class_values)\n",
    "train_item_features = sp.csr_matrix(recommender_utils.get_movies_features(movie_df, train_item_dict, movie_feature_headers, num_feature_headers))\n",
    "train_user_features = sp.csr_matrix(recommender_utils.get_user_features(train_user_idx))\n",
    "val_adjacency_mx, val_labels, val_user_idx, val_item_idx, val_item_dict = recommender_utils.preprocess_data_to_graph(data_array_val, dtypes=recommender_utils.dtypes, class_values=recommender_utils.class_values)\n",
    "val_item_features = sp.csr_matrix(recommender_utils.get_movies_features(movie_df, val_item_dict, movie_feature_headers, num_feature_headers))\n",
    "val_user_features = sp.csr_matrix(recommender_utils.get_user_features(val_user_idx))\n",
    "test_adjacency_mx, test_labels, test_user_idx, test_item_idx, test_item_dict = recommender_utils.preprocess_data_to_graph(data_array_test, dtypes=recommender_utils.dtypes, class_values=recommender_utils.class_values)\n",
    "test_item_features = sp.csr_matrix(recommender_utils.get_movies_features(movie_df, test_item_dict, movie_feature_headers, num_feature_headers))\n",
    "test_user_features = sp.csr_matrix(recommender_utils.get_user_features(test_user_idx))\n",
    "\n",
    "print(\"Train item features shape: \"+str(train_item_features.shape))\n",
    "print(\"Validation item features shape: \"+str(val_item_features.shape))\n",
    "print(\"Test item features shape: \"+str(test_item_features.shape))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "%autoreload 2\n",
    "import torch\n",
    "import math\n",
    "from torch.optim import Adam\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import model_GNN as model_gnn\n",
    "from IGMC.util_functions import *\n",
    "from torch_geometric.loader import DataLoader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_features = True\n",
    "train_item_features_array = train_item_features.toarray() if use_features else None\n",
    "train_user_features_array = train_user_features.toarray() if use_features else None\n",
    "test_item_features_array = test_item_features.toarray() if use_features else None\n",
    "test_user_features_array = test_user_features.toarray() if use_features else None\n",
    "# print(train_item_features_array)\n",
    "train_dataset = MyDynamicDataset(root='data_test/processed/train', A=train_adjacency_mx, \n",
    "    links=(train_user_idx, train_item_idx), labels=train_labels, h=1, sample_ratio=1.0, \n",
    "    max_nodes_per_hop=200, u_features=train_user_features_array, v_features=train_item_features_array, class_values=recommender_utils.class_values)\n",
    "test_dataset = MyDynamicDataset(root='data_test/processed/test', A=test_adjacency_mx, \n",
    "    links=(test_user_idx, test_item_idx), labels=test_labels, h=1, sample_ratio=1.0, \n",
    "    max_nodes_per_hop=200, u_features=test_user_features_array, v_features=test_item_features_array, class_values=recommender_utils.class_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "LR = 1e-3\n",
    "EPOCHS = 80\n",
    "BATCH_SIZE = 50\n",
    "LR_DECAY_STEP = 20\n",
    "LR_DECAY_VALUE = 10\n",
    "\n",
    "train_loader = DataLoader(train_dataset, BATCH_SIZE, shuffle=True, num_workers=2)\n",
    "test_loader = DataLoader(test_dataset, BATCH_SIZE, shuffle=False, num_workers=2)\n",
    "\n",
    "device = torch.device(\"cpu\")\n",
    "if use_features:\n",
    "    model = model_gnn.IGMC(side_features=True, n_side_features=train_item_features_array.shape[1])\n",
    "else:\n",
    "    model = model_gnn.IGMC()\n",
    "model.to(device)\n",
    "model.reset_parameters()\n",
    "optimizer = Adam(model.parameters(), lr=LR, weight_decay=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0/1370\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/user/Documents/Nauka/Studia/Magisterskie/Magisterka/Recommender/.venv/lib/python3.9/site-packages/torch_geometric/deprecation.py:12: UserWarning: 'dropout_adj' is deprecated, use 'dropout_edge' instead\n",
      "  warnings.warn(out)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100/1370\n",
      "200/1370\n",
      "300/1370\n",
      "400/1370\n",
      "500/1370\n",
      "600/1370\n",
      "700/1370\n",
      "800/1370\n",
      "900/1370\n",
      "1000/1370\n",
      "1100/1370\n",
      "1200/1370\n",
      "1300/1370\n",
      "epoch 1 ; train loss 1.1001443987354256\n",
      "0/1370\n",
      "100/1370\n",
      "200/1370\n",
      "300/1370\n",
      "400/1370\n",
      "500/1370\n",
      "600/1370\n",
      "700/1370\n",
      "800/1370\n",
      "900/1370\n",
      "1000/1370\n",
      "1100/1370\n",
      "1200/1370\n",
      "1300/1370\n",
      "epoch 2 ; train loss 0.9072378228176979\n",
      "0/1370\n",
      "100/1370\n",
      "200/1370\n",
      "300/1370\n",
      "400/1370\n",
      "500/1370\n",
      "600/1370\n",
      "700/1370\n",
      "800/1370\n",
      "900/1370\n",
      "1000/1370\n",
      "1100/1370\n",
      "1200/1370\n",
      "1300/1370\n",
      "epoch 3 ; train loss 0.8461752984678496\n",
      "0/1370\n",
      "100/1370\n",
      "200/1370\n",
      "300/1370\n",
      "400/1370\n",
      "500/1370\n",
      "600/1370\n",
      "700/1370\n",
      "800/1370\n",
      "900/1370\n",
      "1000/1370\n",
      "1100/1370\n",
      "1200/1370\n",
      "1300/1370\n",
      "epoch 4 ; train loss 0.8148680065737927\n",
      "0/1370\n",
      "100/1370\n",
      "200/1370\n",
      "300/1370\n",
      "400/1370\n",
      "500/1370\n",
      "600/1370\n",
      "700/1370\n",
      "800/1370\n",
      "900/1370\n",
      "1000/1370\n",
      "1100/1370\n",
      "1200/1370\n",
      "1300/1370\n",
      "epoch 5 ; train loss 0.7863330125756504\n",
      "0/1370\n",
      "100/1370\n",
      "200/1370\n",
      "300/1370\n",
      "400/1370\n",
      "500/1370\n",
      "600/1370\n",
      "700/1370\n",
      "800/1370\n",
      "900/1370\n",
      "1000/1370\n",
      "1100/1370\n",
      "1200/1370\n",
      "1300/1370\n",
      "epoch 6 ; train loss 0.7688442564844276\n",
      "0/1370\n",
      "100/1370\n",
      "200/1370\n",
      "300/1370\n",
      "400/1370\n",
      "500/1370\n",
      "600/1370\n",
      "700/1370\n",
      "800/1370\n",
      "900/1370\n",
      "1000/1370\n",
      "1100/1370\n",
      "1200/1370\n",
      "1300/1370\n",
      "epoch 7 ; train loss 0.7602401350230072\n",
      "0/1370\n",
      "100/1370\n",
      "200/1370\n",
      "300/1370\n",
      "400/1370\n",
      "500/1370\n",
      "600/1370\n",
      "700/1370\n",
      "800/1370\n",
      "900/1370\n",
      "1000/1370\n",
      "1100/1370\n",
      "1200/1370\n",
      "1300/1370\n",
      "epoch 8 ; train loss 0.7483535578402475\n",
      "0/1370\n",
      "100/1370\n",
      "200/1370\n",
      "300/1370\n",
      "400/1370\n",
      "500/1370\n",
      "600/1370\n",
      "700/1370\n",
      "800/1370\n",
      "900/1370\n",
      "1000/1370\n",
      "1100/1370\n",
      "1200/1370\n",
      "1300/1370\n",
      "epoch 9 ; train loss 0.7408550414510674\n",
      "0/1370\n",
      "100/1370\n",
      "200/1370\n",
      "300/1370\n",
      "400/1370\n",
      "500/1370\n",
      "600/1370\n",
      "700/1370\n",
      "800/1370\n",
      "900/1370\n",
      "1000/1370\n",
      "1100/1370\n",
      "1200/1370\n",
      "1300/1370\n",
      "epoch 10 ; train loss 0.7353679607694049\n",
      "0/1370\n",
      "100/1370\n",
      "200/1370\n",
      "300/1370\n",
      "400/1370\n",
      "500/1370\n",
      "600/1370\n",
      "700/1370\n",
      "800/1370\n",
      "900/1370\n",
      "1000/1370\n",
      "1100/1370\n",
      "1200/1370\n",
      "1300/1370\n",
      "epoch 11 ; train loss 0.7313022817414262\n",
      "0/1370\n",
      "100/1370\n",
      "200/1370\n",
      "300/1370\n",
      "400/1370\n",
      "500/1370\n",
      "600/1370\n",
      "700/1370\n",
      "800/1370\n",
      "900/1370\n",
      "1000/1370\n",
      "1100/1370\n",
      "1200/1370\n",
      "1300/1370\n",
      "epoch 12 ; train loss 0.7273760349422104\n",
      "0/1370\n",
      "100/1370\n",
      "200/1370\n",
      "300/1370\n",
      "400/1370\n",
      "500/1370\n",
      "600/1370\n",
      "700/1370\n",
      "800/1370\n",
      "900/1370\n",
      "1000/1370\n",
      "1100/1370\n",
      "1200/1370\n",
      "1300/1370\n",
      "epoch 13 ; train loss 0.7273742243733508\n",
      "0/1370\n",
      "100/1370\n",
      "200/1370\n",
      "300/1370\n",
      "400/1370\n",
      "500/1370\n",
      "600/1370\n",
      "700/1370\n",
      "800/1370\n",
      "900/1370\n",
      "1000/1370\n",
      "1100/1370\n",
      "1200/1370\n",
      "1300/1370\n",
      "epoch 14 ; train loss 0.7200802006363146\n",
      "0/1370\n",
      "100/1370\n",
      "200/1370\n",
      "300/1370\n",
      "400/1370\n",
      "500/1370\n",
      "600/1370\n",
      "700/1370\n",
      "800/1370\n",
      "900/1370\n",
      "1000/1370\n",
      "1100/1370\n",
      "1200/1370\n",
      "1300/1370\n",
      "epoch 15 ; train loss 0.7180982616027957\n",
      "0/1370\n",
      "100/1370\n",
      "200/1370\n",
      "300/1370\n",
      "400/1370\n",
      "500/1370\n",
      "600/1370\n",
      "700/1370\n",
      "800/1370\n",
      "900/1370\n",
      "1000/1370\n",
      "1100/1370\n",
      "1200/1370\n",
      "1300/1370\n",
      "epoch 16 ; train loss 0.7152300935799609\n",
      "0/1370\n",
      "100/1370\n",
      "200/1370\n",
      "300/1370\n",
      "400/1370\n",
      "500/1370\n",
      "600/1370\n",
      "700/1370\n",
      "800/1370\n",
      "900/1370\n",
      "1000/1370\n",
      "1100/1370\n",
      "1200/1370\n",
      "1300/1370\n",
      "epoch 17 ; train loss 0.7132350170827275\n",
      "0/1370\n",
      "100/1370\n",
      "200/1370\n",
      "300/1370\n",
      "400/1370\n",
      "500/1370\n",
      "600/1370\n",
      "700/1370\n",
      "800/1370\n",
      "900/1370\n",
      "1000/1370\n",
      "1100/1370\n",
      "1200/1370\n",
      "1300/1370\n",
      "epoch 18 ; train loss 0.715367846988798\n",
      "0/1370\n",
      "100/1370\n",
      "200/1370\n",
      "300/1370\n",
      "400/1370\n",
      "500/1370\n",
      "600/1370\n",
      "700/1370\n",
      "800/1370\n",
      "900/1370\n",
      "1000/1370\n",
      "1100/1370\n",
      "1200/1370\n",
      "1300/1370\n",
      "epoch 19 ; train loss 0.712111812824315\n",
      "0/1370\n",
      "100/1370\n",
      "200/1370\n",
      "300/1370\n",
      "400/1370\n",
      "500/1370\n",
      "600/1370\n",
      "700/1370\n",
      "800/1370\n",
      "900/1370\n",
      "1000/1370\n",
      "1100/1370\n",
      "1200/1370\n",
      "1300/1370\n",
      "epoch 20 ; train loss 0.7108178512659705\n",
      "0/1370\n",
      "100/1370\n",
      "200/1370\n",
      "300/1370\n",
      "400/1370\n",
      "500/1370\n",
      "600/1370\n",
      "700/1370\n",
      "800/1370\n",
      "900/1370\n",
      "1000/1370\n",
      "1100/1370\n",
      "1200/1370\n",
      "1300/1370\n",
      "epoch 21 ; train loss 0.6963743780579773\n",
      "0/1370\n",
      "100/1370\n",
      "200/1370\n",
      "300/1370\n",
      "400/1370\n",
      "500/1370\n",
      "600/1370\n",
      "700/1370\n",
      "800/1370\n",
      "900/1370\n",
      "1000/1370\n",
      "1100/1370\n",
      "1200/1370\n",
      "1300/1370\n",
      "epoch 22 ; train loss 0.6956936552260199\n",
      "0/1370\n",
      "100/1370\n",
      "200/1370\n",
      "300/1370\n",
      "400/1370\n",
      "500/1370\n",
      "600/1370\n",
      "700/1370\n",
      "800/1370\n",
      "900/1370\n",
      "1000/1370\n",
      "1100/1370\n",
      "1200/1370\n",
      "1300/1370\n",
      "epoch 23 ; train loss 0.6952427563349499\n",
      "0/1370\n",
      "100/1370\n",
      "200/1370\n",
      "300/1370\n",
      "400/1370\n",
      "500/1370\n",
      "600/1370\n",
      "700/1370\n",
      "800/1370\n",
      "900/1370\n",
      "1000/1370\n",
      "1100/1370\n",
      "1200/1370\n",
      "1300/1370\n",
      "epoch 24 ; train loss 0.693791726595538\n",
      "0/1370\n",
      "100/1370\n",
      "200/1370\n",
      "300/1370\n",
      "400/1370\n",
      "500/1370\n",
      "600/1370\n",
      "700/1370\n",
      "800/1370\n",
      "900/1370\n",
      "1000/1370\n",
      "1100/1370\n",
      "1200/1370\n",
      "1300/1370\n",
      "epoch 25 ; train loss 0.691430168401082\n",
      "0/1370\n",
      "100/1370\n",
      "200/1370\n",
      "300/1370\n",
      "400/1370\n",
      "500/1370\n",
      "600/1370\n",
      "700/1370\n",
      "800/1370\n",
      "900/1370\n",
      "1000/1370\n",
      "1100/1370\n",
      "1200/1370\n",
      "1300/1370\n",
      "epoch 26 ; train loss 0.6903415922592466\n",
      "0/1370\n",
      "100/1370\n",
      "200/1370\n",
      "300/1370\n",
      "400/1370\n",
      "500/1370\n",
      "600/1370\n",
      "700/1370\n",
      "800/1370\n",
      "900/1370\n",
      "1000/1370\n",
      "1100/1370\n",
      "1200/1370\n",
      "1300/1370\n",
      "epoch 27 ; train loss 0.6904883464197754\n",
      "0/1370\n",
      "100/1370\n",
      "200/1370\n",
      "300/1370\n",
      "400/1370\n",
      "500/1370\n",
      "600/1370\n",
      "700/1370\n",
      "800/1370\n",
      "900/1370\n",
      "1000/1370\n",
      "1100/1370\n",
      "1200/1370\n",
      "1300/1370\n",
      "epoch 28 ; train loss 0.6911080736723753\n",
      "0/1370\n",
      "100/1370\n",
      "200/1370\n",
      "300/1370\n",
      "400/1370\n",
      "500/1370\n",
      "600/1370\n",
      "700/1370\n",
      "800/1370\n",
      "900/1370\n",
      "1000/1370\n",
      "1100/1370\n",
      "1200/1370\n",
      "1300/1370\n",
      "epoch 29 ; train loss 0.6913113582524969\n",
      "0/1370\n",
      "100/1370\n",
      "200/1370\n",
      "300/1370\n",
      "400/1370\n",
      "500/1370\n",
      "600/1370\n",
      "700/1370\n",
      "800/1370\n",
      "900/1370\n",
      "1000/1370\n",
      "1100/1370\n",
      "1200/1370\n",
      "1300/1370\n",
      "epoch 30 ; train loss 0.6923478358818219\n",
      "0/1370\n",
      "100/1370\n",
      "200/1370\n",
      "300/1370\n",
      "400/1370\n",
      "500/1370\n",
      "600/1370\n",
      "700/1370\n",
      "800/1370\n",
      "900/1370\n",
      "1000/1370\n",
      "1100/1370\n",
      "1200/1370\n",
      "1300/1370\n",
      "epoch 31 ; train loss 0.6892955317876837\n",
      "0/1370\n",
      "100/1370\n",
      "200/1370\n",
      "300/1370\n",
      "400/1370\n",
      "500/1370\n",
      "600/1370\n",
      "700/1370\n",
      "800/1370\n",
      "900/1370\n",
      "1000/1370\n",
      "1100/1370\n",
      "1200/1370\n",
      "1300/1370\n",
      "epoch 32 ; train loss 0.6904658188532021\n",
      "0/1370\n",
      "100/1370\n",
      "200/1370\n",
      "300/1370\n",
      "400/1370\n",
      "500/1370\n",
      "600/1370\n",
      "700/1370\n",
      "800/1370\n",
      "900/1370\n",
      "1000/1370\n",
      "1100/1370\n",
      "1200/1370\n",
      "1300/1370\n",
      "epoch 33 ; train loss 0.6900703647692052\n",
      "0/1370\n",
      "100/1370\n",
      "200/1370\n",
      "300/1370\n",
      "400/1370\n",
      "500/1370\n",
      "600/1370\n",
      "700/1370\n",
      "800/1370\n",
      "900/1370\n",
      "1000/1370\n",
      "1100/1370\n",
      "1200/1370\n",
      "1300/1370\n",
      "epoch 34 ; train loss 0.6900375586102867\n",
      "0/1370\n",
      "100/1370\n",
      "200/1370\n",
      "300/1370\n",
      "400/1370\n",
      "500/1370\n",
      "600/1370\n",
      "700/1370\n",
      "800/1370\n",
      "900/1370\n",
      "1000/1370\n",
      "1100/1370\n",
      "1200/1370\n",
      "1300/1370\n",
      "epoch 35 ; train loss 0.6921960097898122\n",
      "0/1370\n",
      "100/1370\n",
      "200/1370\n",
      "300/1370\n",
      "400/1370\n",
      "500/1370\n",
      "600/1370\n",
      "700/1370\n",
      "800/1370\n",
      "900/1370\n",
      "1000/1370\n",
      "1100/1370\n",
      "1200/1370\n",
      "1300/1370\n",
      "epoch 36 ; train loss 0.691103273212009\n",
      "0/1370\n",
      "100/1370\n",
      "200/1370\n",
      "300/1370\n",
      "400/1370\n",
      "500/1370\n",
      "600/1370\n",
      "700/1370\n",
      "800/1370\n",
      "900/1370\n",
      "1000/1370\n",
      "1100/1370\n",
      "1200/1370\n",
      "1300/1370\n",
      "epoch 37 ; train loss 0.6911651000105314\n",
      "0/1370\n",
      "100/1370\n",
      "200/1370\n",
      "300/1370\n",
      "400/1370\n",
      "500/1370\n",
      "600/1370\n",
      "700/1370\n",
      "800/1370\n",
      "900/1370\n",
      "1000/1370\n",
      "1100/1370\n",
      "1200/1370\n",
      "1300/1370\n",
      "epoch 38 ; train loss 0.6873353684859621\n",
      "0/1370\n",
      "100/1370\n",
      "200/1370\n",
      "300/1370\n",
      "400/1370\n",
      "500/1370\n",
      "600/1370\n",
      "700/1370\n",
      "800/1370\n",
      "900/1370\n",
      "1000/1370\n",
      "1100/1370\n",
      "1200/1370\n",
      "1300/1370\n",
      "epoch 39 ; train loss 0.6884862766778246\n",
      "0/1370\n",
      "100/1370\n",
      "200/1370\n",
      "300/1370\n",
      "400/1370\n",
      "500/1370\n",
      "600/1370\n",
      "700/1370\n",
      "800/1370\n",
      "900/1370\n",
      "1000/1370\n",
      "1100/1370\n",
      "1200/1370\n",
      "1300/1370\n",
      "epoch 40 ; train loss 0.6887967074054011\n",
      "0/1370\n",
      "100/1370\n",
      "200/1370\n",
      "300/1370\n",
      "400/1370\n",
      "500/1370\n",
      "600/1370\n",
      "700/1370\n",
      "800/1370\n",
      "900/1370\n",
      "1000/1370\n",
      "1100/1370\n",
      "1200/1370\n",
      "1300/1370\n",
      "epoch 41 ; train loss 0.6879215444019305\n",
      "0/1370\n",
      "100/1370\n",
      "200/1370\n",
      "300/1370\n",
      "400/1370\n",
      "500/1370\n",
      "600/1370\n",
      "700/1370\n",
      "800/1370\n",
      "900/1370\n",
      "1000/1370\n",
      "1100/1370\n",
      "1200/1370\n",
      "1300/1370\n",
      "epoch 42 ; train loss 0.6860203370402979\n",
      "0/1370\n",
      "100/1370\n",
      "200/1370\n",
      "300/1370\n",
      "400/1370\n",
      "500/1370\n",
      "600/1370\n",
      "700/1370\n",
      "800/1370\n",
      "900/1370\n",
      "1000/1370\n",
      "1100/1370\n",
      "1200/1370\n",
      "1300/1370\n",
      "epoch 43 ; train loss 0.6894578901257199\n",
      "0/1370\n",
      "100/1370\n",
      "200/1370\n",
      "300/1370\n",
      "400/1370\n",
      "500/1370\n",
      "600/1370\n",
      "700/1370\n",
      "800/1370\n",
      "900/1370\n",
      "1000/1370\n",
      "1100/1370\n",
      "1200/1370\n",
      "1300/1370\n",
      "epoch 44 ; train loss 0.6869673246414577\n",
      "0/1370\n",
      "100/1370\n",
      "200/1370\n",
      "300/1370\n",
      "400/1370\n",
      "500/1370\n",
      "600/1370\n",
      "700/1370\n",
      "800/1370\n",
      "900/1370\n",
      "1000/1370\n",
      "1100/1370\n",
      "1200/1370\n",
      "1300/1370\n",
      "epoch 45 ; train loss 0.68845363557491\n",
      "0/1370\n",
      "100/1370\n",
      "200/1370\n",
      "300/1370\n",
      "400/1370\n",
      "500/1370\n",
      "600/1370\n",
      "700/1370\n",
      "800/1370\n",
      "900/1370\n",
      "1000/1370\n",
      "1100/1370\n",
      "1200/1370\n",
      "1300/1370\n",
      "epoch 46 ; train loss 0.6888249229428473\n",
      "0/1370\n",
      "100/1370\n",
      "200/1370\n",
      "300/1370\n",
      "400/1370\n",
      "500/1370\n",
      "600/1370\n",
      "700/1370\n",
      "800/1370\n",
      "900/1370\n",
      "1000/1370\n",
      "1100/1370\n",
      "1200/1370\n",
      "1300/1370\n",
      "epoch 47 ; train loss 0.6869762793702187\n",
      "0/1370\n",
      "100/1370\n",
      "200/1370\n",
      "300/1370\n",
      "400/1370\n",
      "500/1370\n",
      "600/1370\n",
      "700/1370\n",
      "800/1370\n",
      "900/1370\n",
      "1000/1370\n",
      "1100/1370\n",
      "1200/1370\n",
      "1300/1370\n",
      "epoch 48 ; train loss 0.6892692899606516\n",
      "0/1370\n",
      "100/1370\n",
      "200/1370\n",
      "300/1370\n",
      "400/1370\n",
      "500/1370\n",
      "600/1370\n",
      "700/1370\n",
      "800/1370\n",
      "900/1370\n",
      "1000/1370\n",
      "1100/1370\n",
      "1200/1370\n",
      "1300/1370\n",
      "epoch 49 ; train loss 0.6856106951450969\n",
      "0/1370\n",
      "100/1370\n",
      "200/1370\n",
      "300/1370\n",
      "400/1370\n",
      "500/1370\n",
      "600/1370\n",
      "700/1370\n",
      "800/1370\n",
      "900/1370\n",
      "1000/1370\n",
      "1100/1370\n",
      "1200/1370\n",
      "1300/1370\n",
      "epoch 50 ; train loss 0.6845093839872892\n",
      "0/1370\n",
      "100/1370\n",
      "200/1370\n",
      "300/1370\n",
      "400/1370\n",
      "500/1370\n",
      "600/1370\n",
      "700/1370\n",
      "800/1370\n",
      "900/1370\n",
      "1000/1370\n",
      "1100/1370\n",
      "1200/1370\n",
      "1300/1370\n",
      "epoch 51 ; train loss 0.6868446814604019\n",
      "0/1370\n",
      "100/1370\n",
      "200/1370\n",
      "300/1370\n",
      "400/1370\n",
      "500/1370\n",
      "600/1370\n",
      "700/1370\n",
      "800/1370\n",
      "900/1370\n",
      "1000/1370\n",
      "1100/1370\n",
      "1200/1370\n",
      "1300/1370\n",
      "epoch 52 ; train loss 0.6849630511940914\n",
      "0/1370\n",
      "100/1370\n",
      "200/1370\n",
      "300/1370\n",
      "400/1370\n",
      "500/1370\n",
      "600/1370\n",
      "700/1370\n",
      "800/1370\n",
      "900/1370\n",
      "1000/1370\n",
      "1100/1370\n",
      "1200/1370\n",
      "1300/1370\n",
      "epoch 53 ; train loss 0.6874824466561913\n",
      "0/1370\n",
      "100/1370\n",
      "200/1370\n",
      "300/1370\n",
      "400/1370\n",
      "500/1370\n",
      "600/1370\n",
      "700/1370\n",
      "800/1370\n",
      "900/1370\n",
      "1000/1370\n",
      "1100/1370\n",
      "1200/1370\n",
      "1300/1370\n",
      "epoch 54 ; train loss 0.6852364562290307\n",
      "0/1370\n",
      "100/1370\n",
      "200/1370\n",
      "300/1370\n",
      "400/1370\n",
      "500/1370\n",
      "600/1370\n",
      "700/1370\n",
      "800/1370\n",
      "900/1370\n",
      "1000/1370\n",
      "1100/1370\n",
      "1200/1370\n",
      "1300/1370\n",
      "epoch 55 ; train loss 0.6855492760808078\n",
      "0/1370\n",
      "100/1370\n",
      "200/1370\n",
      "300/1370\n",
      "400/1370\n",
      "500/1370\n",
      "600/1370\n",
      "700/1370\n",
      "800/1370\n",
      "900/1370\n",
      "1000/1370\n",
      "1100/1370\n",
      "1200/1370\n",
      "1300/1370\n",
      "epoch 56 ; train loss 0.687499964935117\n",
      "0/1370\n",
      "100/1370\n",
      "200/1370\n",
      "300/1370\n",
      "400/1370\n",
      "500/1370\n",
      "600/1370\n",
      "700/1370\n",
      "800/1370\n",
      "900/1370\n",
      "1000/1370\n",
      "1100/1370\n",
      "1200/1370\n",
      "1300/1370\n",
      "epoch 57 ; train loss 0.6833187913022709\n",
      "0/1370\n",
      "100/1370\n",
      "200/1370\n",
      "300/1370\n",
      "400/1370\n",
      "500/1370\n",
      "600/1370\n",
      "700/1370\n",
      "800/1370\n",
      "900/1370\n",
      "1000/1370\n",
      "1100/1370\n",
      "1200/1370\n",
      "1300/1370\n",
      "epoch 58 ; train loss 0.6864694940651391\n",
      "0/1370\n",
      "100/1370\n",
      "200/1370\n",
      "300/1370\n",
      "400/1370\n",
      "500/1370\n",
      "600/1370\n",
      "700/1370\n",
      "800/1370\n",
      "900/1370\n",
      "1000/1370\n",
      "1100/1370\n",
      "1200/1370\n",
      "1300/1370\n",
      "epoch 59 ; train loss 0.6861749984320171\n",
      "0/1370\n",
      "100/1370\n",
      "200/1370\n",
      "300/1370\n",
      "400/1370\n",
      "500/1370\n",
      "600/1370\n",
      "700/1370\n",
      "800/1370\n",
      "900/1370\n",
      "1000/1370\n",
      "1100/1370\n",
      "1200/1370\n",
      "1300/1370\n",
      "epoch 60 ; train loss 0.6855136106262956\n",
      "0/1370\n",
      "100/1370\n",
      "200/1370\n",
      "300/1370\n",
      "400/1370\n",
      "500/1370\n",
      "600/1370\n",
      "700/1370\n",
      "800/1370\n",
      "900/1370\n",
      "1000/1370\n",
      "1100/1370\n",
      "1200/1370\n",
      "1300/1370\n",
      "epoch 61 ; train loss 0.6851416144225535\n",
      "0/1370\n",
      "100/1370\n",
      "200/1370\n",
      "300/1370\n",
      "400/1370\n",
      "500/1370\n",
      "600/1370\n",
      "700/1370\n",
      "800/1370\n",
      "900/1370\n",
      "1000/1370\n",
      "1100/1370\n",
      "1200/1370\n",
      "1300/1370\n",
      "epoch 62 ; train loss 0.6861921388682279\n",
      "0/1370\n",
      "100/1370\n",
      "200/1370\n",
      "300/1370\n",
      "400/1370\n",
      "500/1370\n",
      "600/1370\n",
      "700/1370\n",
      "800/1370\n",
      "900/1370\n",
      "1000/1370\n",
      "1100/1370\n",
      "1200/1370\n",
      "1300/1370\n",
      "epoch 63 ; train loss 0.6872522820955785\n",
      "0/1370\n",
      "100/1370\n",
      "200/1370\n",
      "300/1370\n",
      "400/1370\n",
      "500/1370\n",
      "600/1370\n",
      "700/1370\n",
      "800/1370\n",
      "900/1370\n",
      "1000/1370\n",
      "1100/1370\n",
      "1200/1370\n",
      "1300/1370\n",
      "epoch 64 ; train loss 0.6832617516934989\n",
      "0/1370\n",
      "100/1370\n",
      "200/1370\n",
      "300/1370\n",
      "400/1370\n",
      "500/1370\n",
      "600/1370\n",
      "700/1370\n",
      "800/1370\n",
      "900/1370\n",
      "1000/1370\n",
      "1100/1370\n",
      "1200/1370\n",
      "1300/1370\n",
      "epoch 65 ; train loss 0.6886272867175469\n",
      "0/1370\n",
      "100/1370\n",
      "200/1370\n",
      "300/1370\n",
      "400/1370\n",
      "500/1370\n",
      "600/1370\n",
      "700/1370\n",
      "800/1370\n",
      "900/1370\n",
      "1000/1370\n",
      "1100/1370\n",
      "1200/1370\n",
      "1300/1370\n",
      "epoch 66 ; train loss 0.6855831750640644\n",
      "0/1370\n",
      "100/1370\n",
      "200/1370\n",
      "300/1370\n",
      "400/1370\n",
      "500/1370\n",
      "600/1370\n",
      "700/1370\n",
      "800/1370\n",
      "900/1370\n",
      "1000/1370\n",
      "1100/1370\n",
      "1200/1370\n",
      "1300/1370\n",
      "epoch 67 ; train loss 0.6856177796089821\n",
      "0/1370\n",
      "100/1370\n",
      "200/1370\n",
      "300/1370\n",
      "400/1370\n",
      "500/1370\n",
      "600/1370\n",
      "700/1370\n",
      "800/1370\n",
      "900/1370\n",
      "1000/1370\n",
      "1100/1370\n",
      "1200/1370\n",
      "1300/1370\n",
      "epoch 68 ; train loss 0.6851089011361907\n",
      "0/1370\n",
      "100/1370\n",
      "200/1370\n",
      "300/1370\n",
      "400/1370\n",
      "500/1370\n",
      "600/1370\n",
      "700/1370\n",
      "800/1370\n",
      "900/1370\n",
      "1000/1370\n",
      "1100/1370\n",
      "1200/1370\n",
      "1300/1370\n",
      "epoch 69 ; train loss 0.6877193237553636\n",
      "0/1370\n",
      "100/1370\n",
      "200/1370\n",
      "300/1370\n",
      "400/1370\n",
      "500/1370\n",
      "600/1370\n",
      "700/1370\n",
      "800/1370\n",
      "900/1370\n",
      "1000/1370\n",
      "1100/1370\n",
      "1200/1370\n",
      "1300/1370\n",
      "epoch 70 ; train loss 0.6839775174249219\n",
      "0/1370\n",
      "100/1370\n",
      "200/1370\n",
      "300/1370\n",
      "400/1370\n",
      "500/1370\n",
      "600/1370\n",
      "700/1370\n",
      "800/1370\n",
      "900/1370\n",
      "1000/1370\n",
      "1100/1370\n",
      "1200/1370\n",
      "1300/1370\n",
      "epoch 71 ; train loss 0.6871925626708536\n",
      "0/1370\n",
      "100/1370\n",
      "200/1370\n",
      "300/1370\n",
      "400/1370\n",
      "500/1370\n",
      "600/1370\n",
      "700/1370\n",
      "800/1370\n",
      "900/1370\n",
      "1000/1370\n",
      "1100/1370\n",
      "1200/1370\n",
      "1300/1370\n",
      "epoch 72 ; train loss 0.6839815796148221\n",
      "0/1370\n",
      "100/1370\n",
      "200/1370\n",
      "300/1370\n",
      "400/1370\n",
      "500/1370\n",
      "600/1370\n",
      "700/1370\n",
      "800/1370\n",
      "900/1370\n",
      "1000/1370\n",
      "1100/1370\n",
      "1200/1370\n",
      "1300/1370\n",
      "epoch 73 ; train loss 0.6879073964895945\n",
      "0/1370\n",
      "100/1370\n",
      "200/1370\n",
      "300/1370\n",
      "400/1370\n",
      "500/1370\n",
      "600/1370\n",
      "700/1370\n",
      "800/1370\n",
      "900/1370\n",
      "1000/1370\n",
      "1100/1370\n",
      "1200/1370\n",
      "1300/1370\n",
      "epoch 74 ; train loss 0.6863563708649368\n",
      "0/1370\n",
      "100/1370\n",
      "200/1370\n",
      "300/1370\n",
      "400/1370\n",
      "500/1370\n",
      "600/1370\n",
      "700/1370\n",
      "800/1370\n",
      "900/1370\n",
      "1000/1370\n",
      "1100/1370\n",
      "1200/1370\n",
      "1300/1370\n",
      "epoch 75 ; train loss 0.6843039099513479\n",
      "0/1370\n",
      "100/1370\n",
      "200/1370\n",
      "300/1370\n",
      "400/1370\n",
      "500/1370\n",
      "600/1370\n",
      "700/1370\n",
      "800/1370\n",
      "900/1370\n",
      "1000/1370\n",
      "1100/1370\n",
      "1200/1370\n",
      "1300/1370\n",
      "epoch 76 ; train loss 0.6866091408645317\n",
      "0/1370\n",
      "100/1370\n",
      "200/1370\n",
      "300/1370\n",
      "400/1370\n",
      "500/1370\n",
      "600/1370\n",
      "700/1370\n",
      "800/1370\n",
      "900/1370\n",
      "1000/1370\n",
      "1100/1370\n",
      "1200/1370\n",
      "1300/1370\n",
      "epoch 77 ; train loss 0.6844961286435224\n",
      "0/1370\n",
      "100/1370\n",
      "200/1370\n",
      "300/1370\n",
      "400/1370\n",
      "500/1370\n",
      "600/1370\n",
      "700/1370\n",
      "800/1370\n",
      "900/1370\n",
      "1000/1370\n",
      "1100/1370\n",
      "1200/1370\n",
      "1300/1370\n",
      "epoch 78 ; train loss 0.6852358136484071\n",
      "0/1370\n",
      "100/1370\n",
      "200/1370\n",
      "300/1370\n",
      "400/1370\n",
      "500/1370\n",
      "600/1370\n",
      "700/1370\n",
      "800/1370\n",
      "900/1370\n",
      "1000/1370\n",
      "1100/1370\n",
      "1200/1370\n",
      "1300/1370\n",
      "epoch 79 ; train loss 0.6882478956480393\n",
      "0/1370\n",
      "100/1370\n",
      "200/1370\n",
      "300/1370\n",
      "400/1370\n",
      "500/1370\n",
      "600/1370\n",
      "700/1370\n",
      "800/1370\n",
      "900/1370\n",
      "1000/1370\n",
      "1100/1370\n",
      "1200/1370\n",
      "1300/1370\n",
      "epoch 80 ; train loss 0.6857797949451088\n"
     ]
    }
   ],
   "source": [
    "loss_through_epochs = []\n",
    "batches_per_epoch = len(train_loader)\n",
    "for epoch in range(1, EPOCHS+1):\n",
    "    model.train()\n",
    "    train_loss_all = 0\n",
    "    for i, train_batch in enumerate(train_loader):\n",
    "        if i % 100 == 0 or i % batches_per_epoch == 0:\n",
    "            print(f\"{i}/{batches_per_epoch}\")\n",
    "        optimizer.zero_grad()\n",
    "        train_batch = train_batch.to(device)\n",
    "        y_pred = model(train_batch)\n",
    "        y_true = train_batch.y\n",
    "        train_loss = F.mse_loss(y_pred, y_true)\n",
    "        train_loss.backward()\n",
    "        train_loss_all += BATCH_SIZE * float(train_loss)\n",
    "        optimizer.step()\n",
    "        torch.cuda.empty_cache()\n",
    "    train_loss_all = train_loss_all / len(train_loader.dataset)\n",
    "    loss_through_epochs.append(train_loss_all)\n",
    "    print('epoch', epoch,'; train loss', train_loss_all)\n",
    "\n",
    "    if epoch % LR_DECAY_STEP == 0:\n",
    "      for param_group in optimizer.param_groups:\n",
    "          param_group['lr'] = param_group['lr'] / LR_DECAY_VALUE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0/302\n",
      "1/302\n",
      "2/302\n",
      "3/302\n",
      "4/302\n",
      "5/302\n",
      "6/302\n",
      "7/302\n",
      "8/302\n",
      "9/302\n",
      "10/302\n",
      "11/302\n",
      "12/302\n",
      "13/302\n",
      "14/302\n",
      "15/302\n",
      "16/302\n",
      "17/302\n",
      "18/302\n",
      "19/302\n",
      "20/302\n",
      "21/302\n",
      "22/302\n",
      "23/302\n",
      "24/302\n",
      "25/302\n",
      "26/302\n",
      "27/302\n",
      "28/302\n",
      "29/302\n",
      "30/302\n",
      "31/302\n",
      "32/302\n",
      "33/302\n",
      "34/302\n",
      "35/302\n",
      "36/302\n",
      "37/302\n",
      "38/302\n",
      "39/302\n",
      "40/302\n",
      "41/302\n",
      "42/302\n",
      "43/302\n",
      "44/302\n",
      "45/302\n",
      "46/302\n",
      "47/302\n",
      "48/302\n",
      "49/302\n",
      "50/302\n",
      "51/302\n",
      "52/302\n",
      "53/302\n",
      "54/302\n",
      "55/302\n",
      "56/302\n",
      "57/302\n",
      "58/302\n",
      "59/302\n",
      "60/302\n",
      "61/302\n",
      "62/302\n",
      "63/302\n",
      "64/302\n",
      "65/302\n",
      "66/302\n",
      "67/302\n",
      "68/302\n",
      "69/302\n",
      "70/302\n",
      "71/302\n",
      "72/302\n",
      "73/302\n",
      "74/302\n",
      "75/302\n",
      "76/302\n",
      "77/302\n",
      "78/302\n",
      "79/302\n",
      "80/302\n",
      "81/302\n",
      "82/302\n",
      "83/302\n",
      "84/302\n",
      "85/302\n",
      "86/302\n",
      "87/302\n",
      "88/302\n",
      "89/302\n",
      "90/302\n",
      "91/302\n",
      "92/302\n",
      "93/302\n",
      "94/302\n",
      "95/302\n",
      "96/302\n",
      "97/302\n",
      "98/302\n",
      "99/302\n",
      "100/302\n",
      "101/302\n",
      "102/302\n",
      "103/302\n",
      "104/302\n",
      "105/302\n",
      "106/302\n",
      "107/302\n",
      "108/302\n",
      "109/302\n",
      "110/302\n",
      "111/302\n",
      "112/302\n",
      "113/302\n",
      "114/302\n",
      "115/302\n",
      "116/302\n",
      "117/302\n",
      "118/302\n",
      "119/302\n",
      "120/302\n",
      "121/302\n",
      "122/302\n",
      "123/302\n",
      "124/302\n",
      "125/302\n",
      "126/302\n",
      "127/302\n",
      "128/302\n",
      "129/302\n",
      "130/302\n",
      "131/302\n",
      "132/302\n",
      "133/302\n",
      "134/302\n",
      "135/302\n",
      "136/302\n",
      "137/302\n",
      "138/302\n",
      "139/302\n",
      "140/302\n",
      "141/302\n",
      "142/302\n",
      "143/302\n",
      "144/302\n",
      "145/302\n",
      "146/302\n",
      "147/302\n",
      "148/302\n",
      "149/302\n",
      "150/302\n",
      "151/302\n",
      "152/302\n",
      "153/302\n",
      "154/302\n",
      "155/302\n",
      "156/302\n",
      "157/302\n",
      "158/302\n",
      "159/302\n",
      "160/302\n",
      "161/302\n",
      "162/302\n",
      "163/302\n",
      "164/302\n",
      "165/302\n",
      "166/302\n",
      "167/302\n",
      "168/302\n",
      "169/302\n",
      "170/302\n",
      "171/302\n",
      "172/302\n",
      "173/302\n",
      "174/302\n",
      "175/302\n",
      "176/302\n",
      "177/302\n",
      "178/302\n",
      "179/302\n",
      "180/302\n",
      "181/302\n",
      "182/302\n",
      "183/302\n",
      "184/302\n",
      "185/302\n",
      "186/302\n",
      "187/302\n",
      "188/302\n",
      "189/302\n",
      "190/302\n",
      "191/302\n",
      "192/302\n",
      "193/302\n",
      "194/302\n",
      "195/302\n",
      "196/302\n",
      "197/302\n",
      "198/302\n",
      "199/302\n",
      "200/302\n",
      "201/302\n",
      "202/302\n",
      "203/302\n",
      "204/302\n",
      "205/302\n",
      "206/302\n",
      "207/302\n",
      "208/302\n",
      "209/302\n",
      "210/302\n",
      "211/302\n",
      "212/302\n",
      "213/302\n",
      "214/302\n",
      "215/302\n",
      "216/302\n",
      "217/302\n",
      "218/302\n",
      "219/302\n",
      "220/302\n",
      "221/302\n",
      "222/302\n",
      "223/302\n",
      "224/302\n",
      "225/302\n",
      "226/302\n",
      "227/302\n",
      "228/302\n",
      "229/302\n",
      "230/302\n",
      "231/302\n",
      "232/302\n",
      "233/302\n",
      "234/302\n",
      "235/302\n",
      "236/302\n",
      "237/302\n",
      "238/302\n",
      "239/302\n",
      "240/302\n",
      "241/302\n",
      "242/302\n",
      "243/302\n",
      "244/302\n",
      "245/302\n",
      "246/302\n",
      "247/302\n",
      "248/302\n",
      "249/302\n",
      "250/302\n",
      "251/302\n",
      "252/302\n",
      "253/302\n",
      "254/302\n",
      "255/302\n",
      "256/302\n",
      "257/302\n",
      "258/302\n",
      "259/302\n",
      "260/302\n",
      "261/302\n",
      "262/302\n",
      "263/302\n",
      "264/302\n",
      "265/302\n",
      "266/302\n",
      "267/302\n",
      "268/302\n",
      "269/302\n",
      "270/302\n",
      "271/302\n",
      "272/302\n",
      "273/302\n",
      "274/302\n",
      "275/302\n",
      "276/302\n",
      "277/302\n",
      "278/302\n",
      "279/302\n",
      "280/302\n",
      "281/302\n",
      "282/302\n",
      "283/302\n",
      "284/302\n",
      "285/302\n",
      "286/302\n",
      "287/302\n",
      "288/302\n",
      "289/302\n",
      "290/302\n",
      "291/302\n",
      "292/302\n",
      "293/302\n",
      "294/302\n",
      "295/302\n",
      "296/302\n",
      "297/302\n",
      "298/302\n",
      "299/302\n",
      "300/302\n",
      "301/302\n",
      "test MSE loss 0.7541465488264392\n",
      "test RMSE loss 0.8684161150200054\n"
     ]
    }
   ],
   "source": [
    "batches_in_eval = len(test_loader)\n",
    "model.eval()\n",
    "test_loss = 0\n",
    "for i, test_batch in enumerate(test_loader):\n",
    "    print(f\"{i}/{batches_in_eval}\")\n",
    "    test_batch = test_batch.to(device)\n",
    "    with torch.no_grad():\n",
    "        y_pred = model(test_batch)\n",
    "    y_true = test_batch.y\n",
    "    test_loss += F.mse_loss(y_pred, y_true, reduction='sum')\n",
    "    torch.cuda.empty_cache()\n",
    "mse_loss = float(test_loss) / len(test_loader.dataset)\n",
    "\n",
    "print('test MSE loss', mse_loss)\n",
    "print('test RMSE loss', math.sqrt(mse_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"models/graph_80epochs_with_features_18_genres_mean_unb_popularity.pt\")\n",
    "epochs_80_with_20_features = loss_through_epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.1001443987354256, 0.9072378228176979, 0.8461752984678496, 0.8148680065737927, 0.7863330125756504, 0.7688442564844276, 0.7602401350230072, 0.7483535578402475, 0.7408550414510674, 0.7353679607694049, 0.7313022817414262, 0.7273760349422104, 0.7273742243733508, 0.7200802006363146, 0.7180982616027957, 0.7152300935799609, 0.7132350170827275, 0.715367846988798, 0.712111812824315, 0.7108178512659705, 0.6963743780579773, 0.6956936552260199, 0.6952427563349499, 0.693791726595538, 0.691430168401082, 0.6903415922592466, 0.6904883464197754, 0.6911080736723753, 0.6913113582524969, 0.6923478358818219, 0.6892955317876837, 0.6904658188532021, 0.6900703647692052, 0.6900375586102867, 0.6921960097898122, 0.691103273212009, 0.6911651000105314, 0.6873353684859621, 0.6884862766778246, 0.6887967074054011, 0.6879215444019305, 0.6860203370402979, 0.6894578901257199, 0.6869673246414577, 0.68845363557491, 0.6888249229428473, 0.6869762793702187, 0.6892692899606516, 0.6856106951450969, 0.6845093839872892, 0.6868446814604019, 0.6849630511940914, 0.6874824466561913, 0.6852364562290307, 0.6855492760808078, 0.687499964935117, 0.6833187913022709, 0.6864694940651391, 0.6861749984320171, 0.6855136106262956, 0.6851416144225535, 0.6861921388682279, 0.6872522820955785, 0.6832617516934989, 0.6886272867175469, 0.6855831750640644, 0.6856177796089821, 0.6851089011361907, 0.6877193237553636, 0.6839775174249219, 0.6871925626708536, 0.6839815796148221, 0.6879073964895945, 0.6863563708649368, 0.6843039099513479, 0.6866091408645317, 0.6844961286435224, 0.6852358136484071, 0.6882478956480393, 0.6857797949451088]\n"
     ]
    }
   ],
   "source": [
    "print(epochs_80_with_20_features)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6 (default, Sep 26 2022, 11:37:49) \n[Clang 14.0.0 (clang-1400.0.29.202)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "f24bbe3eee7f1671aa96cb4424e1ae784be864432abf663e2856784d4fd29d28"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
