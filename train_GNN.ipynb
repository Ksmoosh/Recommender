{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import recommender_utils\n",
    "import numpy as np\n",
    "import scipy.sparse as sp\n",
    "import pandas as pd\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train item features shape: (8500, 22)\n",
      "Validation item features shape: (5536, 22)\n",
      "Test item features shape: (3975, 22)\n"
     ]
    }
   ],
   "source": [
    "movie_df, movie_feature_headers, num_feature_headers = recommender_utils.get_movies_data(filepath='data_small/movies.csv', separator=r',', movies_columns_to_drop=['genres'], genres=True, other_features=[3,4])\n",
    "\n",
    "data_train = recommender_utils.get_ratings_data(filepath='data_small/train.csv', separator=r',', dtypes=recommender_utils.dtypes)\n",
    "data_val = recommender_utils.get_ratings_data(filepath='data_small/validate.csv', separator=r',', dtypes=recommender_utils.dtypes)\n",
    "data_test = recommender_utils.get_ratings_data(filepath='data_small/test.csv', separator=r',', dtypes=recommender_utils.dtypes)\n",
    "\n",
    "data_array_train = np.array(data_train.values.tolist())\n",
    "data_array_val = np.array(data_val.values.tolist())\n",
    "data_array_test = np.array(data_test.values.tolist())\n",
    "\n",
    "# add features for whole dataset TODO\n",
    "train_adjacency_mx, train_labels, train_user_idx, train_item_idx, train_item_dict = recommender_utils.preprocess_data_to_graph(data_array_train, dtypes=recommender_utils.dtypes, class_values=recommender_utils.class_values)\n",
    "train_item_features = sp.csr_matrix(recommender_utils.get_movies_features(movie_df, train_item_dict, movie_feature_headers, num_feature_headers))\n",
    "train_user_features = sp.csr_matrix(recommender_utils.get_user_features(train_user_idx))\n",
    "val_adjacency_mx, val_labels, val_user_idx, val_item_idx, val_item_dict = recommender_utils.preprocess_data_to_graph(data_array_val, dtypes=recommender_utils.dtypes, class_values=recommender_utils.class_values)\n",
    "val_item_features = sp.csr_matrix(recommender_utils.get_movies_features(movie_df, val_item_dict, movie_feature_headers, num_feature_headers))\n",
    "val_user_features = sp.csr_matrix(recommender_utils.get_user_features(val_user_idx))\n",
    "test_adjacency_mx, test_labels, test_user_idx, test_item_idx, test_item_dict = recommender_utils.preprocess_data_to_graph(data_array_test, dtypes=recommender_utils.dtypes, class_values=recommender_utils.class_values)\n",
    "test_item_features = sp.csr_matrix(recommender_utils.get_movies_features(movie_df, test_item_dict, movie_feature_headers, num_feature_headers))\n",
    "test_user_features = sp.csr_matrix(recommender_utils.get_user_features(test_user_idx))\n",
    "\n",
    "print(\"Train item features shape: \"+str(train_item_features.shape))\n",
    "print(\"Validation item features shape: \"+str(val_item_features.shape))\n",
    "print(\"Test item features shape: \"+str(test_item_features.shape))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['popularity', 'mean_unbiased', '(no genres listed)', 'Action',\n",
       "       'Adventure', 'Animation', 'Children', 'Comedy', 'Crime',\n",
       "       'Documentary', 'Drama', 'Fantasy', 'Film-Noir', 'Horror', 'IMAX',\n",
       "       'Musical', 'Mystery', 'Romance', 'Sci-Fi', 'Thriller', 'War',\n",
       "       'Western'], dtype=object)"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "movie_feature_headers\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "%autoreload 2\n",
    "import torch\n",
    "import math\n",
    "from torch.optim import Adam\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import model_GNN as model_gnn\n",
    "from IGMC.util_functions import *\n",
    "from torch_geometric.loader import DataLoader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_features = True\n",
    "train_item_features_array = train_item_features.toarray() if use_features else None\n",
    "train_user_features_array = train_user_features.toarray() if use_features else None\n",
    "test_item_features_array = test_item_features.toarray() if use_features else None\n",
    "test_user_features_array = test_user_features.toarray() if use_features else None\n",
    "val_item_features_array = val_item_features.toarray() if use_features else None\n",
    "val_user_features_array = val_user_features.toarray() if use_features else None\n",
    "\n",
    "train_dataset = MyDynamicDataset(root='data_test/processed/train', A=train_adjacency_mx, \n",
    "    links=(train_user_idx, train_item_idx), labels=train_labels, h=1, sample_ratio=1.0, \n",
    "    max_nodes_per_hop=200, u_features=train_user_features_array, v_features=train_item_features_array, class_values=recommender_utils.class_values)\n",
    "test_dataset = MyDynamicDataset(root='data_test/processed/test', A=test_adjacency_mx, \n",
    "    links=(test_user_idx, test_item_idx), labels=test_labels, h=1, sample_ratio=1.0, \n",
    "    max_nodes_per_hop=200, u_features=test_user_features_array, v_features=test_item_features_array, class_values=recommender_utils.class_values)\n",
    "val_dataset = MyDynamicDataset(root='data_test/processed/val', A=val_adjacency_mx, \n",
    "    links=(val_user_idx, val_item_idx), labels=val_labels, h=1, sample_ratio=1.0, \n",
    "    max_nodes_per_hop=200, u_features=val_user_features_array, v_features=val_item_features_array, class_values=recommender_utils.class_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "LR = 1e-3\n",
    "EPOCHS = 80\n",
    "BATCH_SIZE = 50\n",
    "LR_DECAY_STEP = 20\n",
    "LR_DECAY_VALUE = 10\n",
    "\n",
    "train_loader = DataLoader(train_dataset, BATCH_SIZE, shuffle=True, num_workers=2)\n",
    "val_loader = DataLoader(val_dataset, BATCH_SIZE, shuffle=False, num_workers=2)\n",
    "test_loader = DataLoader(test_dataset, BATCH_SIZE, shuffle=False, num_workers=2)\n",
    "\n",
    "device = torch.device(\"cpu\")\n",
    "if use_features:\n",
    "    model = model_gnn.IGMC(side_features=True, n_side_features=train_item_features_array.shape[1])\n",
    "else:\n",
    "    model = model_gnn.IGMC()\n",
    "model.to(device)\n",
    "model.reset_parameters()\n",
    "optimizer = Adam(model.parameters(), lr=LR, weight_decay=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0/1370\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/user/Documents/Nauka/Studia/Magisterskie/Magisterka/Recommender/.venv/lib/python3.9/site-packages/torch_geometric/deprecation.py:12: UserWarning: 'dropout_adj' is deprecated, use 'dropout_edge' instead\n",
      "  warnings.warn(out)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100/1370\n",
      "200/1370\n",
      "300/1370\n",
      "400/1370\n",
      "500/1370\n",
      "600/1370\n",
      "700/1370\n",
      "800/1370\n",
      "900/1370\n",
      "1000/1370\n",
      "1100/1370\n",
      "1200/1370\n",
      "1300/1370\n",
      "epoch 1 ; train loss 1.0956418637820722\n",
      "0/346\n",
      "100/346\n",
      "200/346\n",
      "300/346\n",
      "epoch 1 ; val loss 0.8760156162531564\n",
      "loss improvement on epoch: 1\n",
      "0/1370\n",
      "100/1370\n",
      "200/1370\n",
      "300/1370\n",
      "400/1370\n",
      "500/1370\n",
      "600/1370\n",
      "700/1370\n",
      "800/1370\n",
      "900/1370\n",
      "1000/1370\n",
      "1100/1370\n",
      "1200/1370\n",
      "1300/1370\n",
      "epoch 2 ; train loss 0.9030332199778294\n",
      "0/346\n",
      "100/346\n",
      "200/346\n",
      "300/346\n",
      "epoch 2 ; val loss 0.8104765158215607\n",
      "loss improvement on epoch: 2\n",
      "0/1370\n",
      "100/1370\n",
      "200/1370\n",
      "300/1370\n",
      "400/1370\n",
      "500/1370\n",
      "600/1370\n",
      "700/1370\n",
      "800/1370\n",
      "900/1370\n",
      "1000/1370\n",
      "1100/1370\n",
      "1200/1370\n",
      "1300/1370\n",
      "epoch 3 ; train loss 0.8599454381620982\n",
      "0/346\n",
      "100/346\n",
      "200/346\n",
      "300/346\n",
      "epoch 3 ; val loss 0.7670360566461804\n",
      "loss improvement on epoch: 3\n",
      "0/1370\n",
      "100/1370\n",
      "200/1370\n",
      "300/1370\n",
      "400/1370\n",
      "500/1370\n",
      "600/1370\n",
      "700/1370\n",
      "800/1370\n",
      "900/1370\n",
      "1000/1370\n",
      "1100/1370\n",
      "1200/1370\n",
      "1300/1370\n",
      "epoch 4 ; train loss 0.8306757568512462\n",
      "0/346\n",
      "100/346\n",
      "200/346\n",
      "300/346\n",
      "epoch 4 ; val loss 0.7469011650041297\n",
      "loss improvement on epoch: 4\n",
      "0/1370\n",
      "100/1370\n",
      "200/1370\n",
      "300/1370\n",
      "400/1370\n",
      "500/1370\n",
      "600/1370\n",
      "700/1370\n",
      "800/1370\n",
      "900/1370\n",
      "1000/1370\n",
      "1100/1370\n",
      "1200/1370\n",
      "1300/1370\n",
      "epoch 5 ; train loss 0.8014144271740522\n",
      "0/346\n",
      "100/346\n",
      "200/346\n",
      "300/346\n",
      "epoch 5 ; val loss 0.7294881486126946\n",
      "loss improvement on epoch: 5\n",
      "0/1370\n",
      "100/1370\n",
      "200/1370\n",
      "300/1370\n",
      "400/1370\n",
      "500/1370\n",
      "600/1370\n",
      "700/1370\n",
      "800/1370\n",
      "900/1370\n",
      "1000/1370\n",
      "1100/1370\n",
      "1200/1370\n",
      "1300/1370\n",
      "epoch 6 ; train loss 0.7744342417419022\n",
      "0/346\n",
      "100/346\n",
      "200/346\n",
      "300/346\n",
      "epoch 6 ; val loss 0.7231862322185878\n",
      "loss improvement on epoch: 6\n",
      "0/1370\n",
      "100/1370\n",
      "200/1370\n",
      "300/1370\n",
      "400/1370\n",
      "500/1370\n",
      "600/1370\n",
      "700/1370\n",
      "800/1370\n",
      "900/1370\n",
      "1000/1370\n",
      "1100/1370\n",
      "1200/1370\n",
      "1300/1370\n",
      "epoch 7 ; train loss 0.7660812228271949\n",
      "0/346\n",
      "100/346\n",
      "200/346\n",
      "300/346\n",
      "epoch 7 ; val loss 0.7048757768374074\n",
      "loss improvement on epoch: 7\n",
      "0/1370\n",
      "100/1370\n",
      "200/1370\n",
      "300/1370\n",
      "400/1370\n",
      "500/1370\n",
      "600/1370\n",
      "700/1370\n",
      "800/1370\n",
      "900/1370\n",
      "1000/1370\n",
      "1100/1370\n",
      "1200/1370\n",
      "1300/1370\n",
      "epoch 8 ; train loss 0.7518042779654532\n",
      "0/346\n",
      "100/346\n",
      "200/346\n",
      "300/346\n",
      "epoch 8 ; val loss 0.7047970893499614\n",
      "loss improvement on epoch: 8\n",
      "0/1370\n",
      "100/1370\n",
      "200/1370\n",
      "300/1370\n",
      "400/1370\n",
      "500/1370\n",
      "600/1370\n",
      "700/1370\n",
      "800/1370\n",
      "900/1370\n",
      "1000/1370\n",
      "1100/1370\n",
      "1200/1370\n",
      "1300/1370\n",
      "epoch 9 ; train loss 0.7470312561855863\n",
      "0/346\n",
      "100/346\n",
      "200/346\n",
      "300/346\n",
      "epoch 9 ; val loss 0.7056272401543403\n",
      "0/1370\n",
      "100/1370\n",
      "200/1370\n",
      "300/1370\n",
      "400/1370\n",
      "500/1370\n",
      "600/1370\n",
      "700/1370\n",
      "800/1370\n",
      "900/1370\n",
      "1000/1370\n",
      "1100/1370\n",
      "1200/1370\n",
      "1300/1370\n",
      "epoch 10 ; train loss 0.7406068221523219\n",
      "0/346\n",
      "100/346\n",
      "200/346\n",
      "300/346\n",
      "epoch 10 ; val loss 0.6955736373654671\n",
      "loss improvement on epoch: 10\n",
      "0/1370\n",
      "100/1370\n",
      "200/1370\n",
      "300/1370\n",
      "400/1370\n",
      "500/1370\n",
      "600/1370\n",
      "700/1370\n",
      "800/1370\n",
      "900/1370\n",
      "1000/1370\n",
      "1100/1370\n",
      "1200/1370\n",
      "1300/1370\n",
      "epoch 11 ; train loss 0.7357250091907569\n",
      "0/346\n",
      "100/346\n",
      "200/346\n",
      "300/346\n",
      "epoch 11 ; val loss 0.6847784185299167\n",
      "loss improvement on epoch: 11\n",
      "0/1370\n",
      "100/1370\n",
      "200/1370\n",
      "300/1370\n",
      "400/1370\n",
      "500/1370\n",
      "600/1370\n",
      "700/1370\n",
      "800/1370\n",
      "900/1370\n",
      "1000/1370\n",
      "1100/1370\n",
      "1200/1370\n",
      "1300/1370\n",
      "epoch 12 ; train loss 0.7326886784691855\n",
      "0/346\n",
      "100/346\n",
      "200/346\n",
      "300/346\n",
      "epoch 12 ; val loss 0.6809563718356744\n",
      "loss improvement on epoch: 12\n",
      "0/1370\n",
      "100/1370\n",
      "200/1370\n",
      "300/1370\n",
      "400/1370\n",
      "500/1370\n",
      "600/1370\n",
      "700/1370\n",
      "800/1370\n",
      "900/1370\n",
      "1000/1370\n",
      "1100/1370\n",
      "1200/1370\n",
      "1300/1370\n",
      "epoch 13 ; train loss 0.7273591589393368\n",
      "0/346\n",
      "100/346\n",
      "200/346\n",
      "300/346\n",
      "epoch 13 ; val loss 0.6776994318028705\n",
      "loss improvement on epoch: 13\n",
      "0/1370\n",
      "100/1370\n",
      "200/1370\n",
      "300/1370\n",
      "400/1370\n",
      "500/1370\n",
      "600/1370\n",
      "700/1370\n",
      "800/1370\n",
      "900/1370\n",
      "1000/1370\n",
      "1100/1370\n",
      "1200/1370\n",
      "1300/1370\n",
      "epoch 14 ; train loss 0.7263968659235671\n",
      "0/346\n",
      "100/346\n",
      "200/346\n",
      "300/346\n",
      "epoch 14 ; val loss 0.6763932560311837\n",
      "loss improvement on epoch: 14\n",
      "0/1370\n",
      "100/1370\n",
      "200/1370\n",
      "300/1370\n",
      "400/1370\n",
      "500/1370\n",
      "600/1370\n",
      "700/1370\n",
      "800/1370\n",
      "900/1370\n",
      "1000/1370\n",
      "1100/1370\n",
      "1200/1370\n",
      "1300/1370\n",
      "epoch 15 ; train loss 0.7240426515356382\n",
      "0/346\n",
      "100/346\n",
      "200/346\n",
      "300/346\n",
      "epoch 15 ; val loss 0.6763293810568198\n",
      "loss improvement on epoch: 15\n",
      "0/1370\n",
      "100/1370\n",
      "200/1370\n",
      "300/1370\n",
      "400/1370\n",
      "500/1370\n",
      "600/1370\n",
      "700/1370\n",
      "800/1370\n",
      "900/1370\n",
      "1000/1370\n",
      "1100/1370\n",
      "1200/1370\n",
      "1300/1370\n",
      "epoch 16 ; train loss 0.7223901849861989\n",
      "0/346\n",
      "100/346\n",
      "200/346\n",
      "300/346\n",
      "epoch 16 ; val loss 0.6723934714251233\n",
      "loss improvement on epoch: 16\n",
      "0/1370\n",
      "100/1370\n",
      "200/1370\n",
      "300/1370\n",
      "400/1370\n",
      "500/1370\n",
      "600/1370\n",
      "700/1370\n",
      "800/1370\n",
      "900/1370\n",
      "1000/1370\n",
      "1100/1370\n",
      "1200/1370\n",
      "1300/1370\n",
      "epoch 17 ; train loss 0.7206002942479712\n",
      "0/346\n",
      "100/346\n",
      "200/346\n",
      "300/346\n",
      "epoch 17 ; val loss 0.6778236435881505\n",
      "0/1370\n",
      "100/1370\n",
      "200/1370\n",
      "300/1370\n",
      "400/1370\n",
      "500/1370\n",
      "600/1370\n",
      "700/1370\n",
      "800/1370\n",
      "900/1370\n",
      "1000/1370\n",
      "1100/1370\n",
      "1200/1370\n",
      "1300/1370\n",
      "epoch 18 ; train loss 0.7198619279507102\n",
      "0/346\n",
      "100/346\n",
      "200/346\n",
      "300/346\n",
      "epoch 18 ; val loss 0.6749805639049521\n",
      "0/1370\n",
      "100/1370\n",
      "200/1370\n",
      "300/1370\n",
      "400/1370\n",
      "500/1370\n",
      "600/1370\n",
      "700/1370\n",
      "800/1370\n",
      "900/1370\n",
      "1000/1370\n",
      "1100/1370\n",
      "1200/1370\n",
      "1300/1370\n",
      "epoch 19 ; train loss 0.7225346576124136\n",
      "0/346\n",
      "100/346\n",
      "200/346\n",
      "300/346\n",
      "epoch 19 ; val loss 0.6858574759853245\n",
      "0/1370\n",
      "100/1370\n",
      "200/1370\n",
      "300/1370\n",
      "400/1370\n",
      "500/1370\n",
      "600/1370\n",
      "700/1370\n",
      "800/1370\n",
      "900/1370\n",
      "1000/1370\n",
      "1100/1370\n",
      "1200/1370\n",
      "1300/1370\n",
      "epoch 20 ; train loss 0.7186695182790755\n",
      "0/346\n",
      "100/346\n",
      "200/346\n",
      "300/346\n",
      "epoch 20 ; val loss 0.6712025444937387\n",
      "loss improvement on epoch: 20\n",
      "0/1370\n",
      "100/1370\n",
      "200/1370\n",
      "300/1370\n",
      "400/1370\n",
      "500/1370\n",
      "600/1370\n",
      "700/1370\n",
      "800/1370\n",
      "900/1370\n",
      "1000/1370\n",
      "1100/1370\n",
      "1200/1370\n",
      "1300/1370\n",
      "epoch 21 ; train loss 0.706117359188597\n",
      "0/346\n",
      "100/346\n",
      "200/346\n",
      "300/346\n",
      "epoch 21 ; val loss 0.6683045371297609\n",
      "loss improvement on epoch: 21\n",
      "0/1370\n",
      "100/1370\n",
      "200/1370\n",
      "300/1370\n",
      "400/1370\n",
      "500/1370\n",
      "600/1370\n",
      "700/1370\n",
      "800/1370\n",
      "900/1370\n",
      "1000/1370\n",
      "1100/1370\n",
      "1200/1370\n",
      "1300/1370\n",
      "epoch 22 ; train loss 0.7038396784437599\n",
      "0/346\n",
      "100/346\n",
      "200/346\n",
      "300/346\n",
      "epoch 22 ; val loss 0.6693794615825431\n",
      "0/1370\n",
      "100/1370\n",
      "200/1370\n",
      "300/1370\n",
      "400/1370\n",
      "500/1370\n",
      "600/1370\n",
      "700/1370\n",
      "800/1370\n",
      "900/1370\n",
      "1000/1370\n",
      "1100/1370\n",
      "1200/1370\n",
      "1300/1370\n",
      "epoch 23 ; train loss 0.7009907672810584\n",
      "0/346\n",
      "100/346\n",
      "200/346\n",
      "300/346\n",
      "epoch 23 ; val loss 0.6741390801552269\n",
      "0/1370\n",
      "100/1370\n",
      "200/1370\n",
      "300/1370\n",
      "400/1370\n",
      "500/1370\n",
      "600/1370\n",
      "700/1370\n",
      "800/1370\n",
      "900/1370\n",
      "1000/1370\n",
      "1100/1370\n",
      "1200/1370\n",
      "1300/1370\n",
      "epoch 24 ; train loss 0.7000008635742315\n",
      "0/346\n",
      "100/346\n",
      "200/346\n",
      "300/346\n",
      "epoch 24 ; val loss 0.6701719814790758\n",
      "0/1370\n",
      "100/1370\n",
      "200/1370\n",
      "300/1370\n",
      "400/1370\n",
      "500/1370\n",
      "600/1370\n",
      "700/1370\n",
      "800/1370\n",
      "900/1370\n",
      "1000/1370\n",
      "1100/1370\n",
      "1200/1370\n",
      "1300/1370\n",
      "epoch 25 ; train loss 0.70006048999587\n",
      "0/346\n",
      "100/346\n",
      "200/346\n",
      "300/346\n",
      "epoch 25 ; val loss 0.6698067337102084\n",
      "0/1370\n",
      "100/1370\n",
      "200/1370\n",
      "300/1370\n",
      "400/1370\n",
      "500/1370\n",
      "600/1370\n",
      "700/1370\n",
      "800/1370\n",
      "900/1370\n",
      "1000/1370\n",
      "1100/1370\n",
      "1200/1370\n",
      "1300/1370\n",
      "epoch 26 ; train loss 0.7011145274998872\n",
      "0/346\n",
      "100/346\n",
      "200/346\n",
      "300/346\n",
      "epoch 26 ; val loss 0.6692571541794611\n",
      "early stopping after epoch 026\n"
     ]
    }
   ],
   "source": [
    "history = []\n",
    "batches_per_epoch_train = len(train_loader)\n",
    "batches_per_epoch_val = len(val_loader)\n",
    "best_weights = None\n",
    "best_loss = np.inf\n",
    "patience = 5\n",
    "no_improvements = 0\n",
    "\n",
    "for epoch in range(1, EPOCHS+1):\n",
    "    stats = {'epoch': epoch, 'total': EPOCHS}\n",
    "    model.train()\n",
    "    train_loss_all = 0\n",
    "    for i, train_batch in enumerate(train_loader):\n",
    "        if i % 100 == 0 or i % batches_per_epoch_train == 0:\n",
    "            print(f\"{i}/{batches_per_epoch_train}\")\n",
    "        optimizer.zero_grad()\n",
    "        train_batch = train_batch.to(device)\n",
    "        y_pred = model(train_batch)\n",
    "        y_true = train_batch.y\n",
    "        train_loss = F.mse_loss(y_pred, y_true)\n",
    "        train_loss.backward()\n",
    "        train_loss_all += BATCH_SIZE * float(train_loss)\n",
    "        optimizer.step()\n",
    "    train_loss_all = train_loss_all / len(train_loader.dataset)\n",
    "    stats[\"train\"] = train_loss_all\n",
    "    print('epoch', epoch,'; train loss', train_loss_all)\n",
    "\n",
    "    val_loss_all = 0\n",
    "    for i, val_batch in enumerate(val_loader):\n",
    "        if i % 100 == 0 or i % batches_per_epoch_val == 0:\n",
    "            print(f\"{i}/{batches_per_epoch_val}\")\n",
    "        optimizer.zero_grad()\n",
    "        val_batch = val_batch.to(device)\n",
    "        y_pred = model(val_batch)\n",
    "        y_true = val_batch.y\n",
    "        val_loss = F.mse_loss(y_pred, y_true)\n",
    "        val_loss_all += BATCH_SIZE * float(val_loss)\n",
    "    val_loss_all = val_loss_all / len(val_loader.dataset)\n",
    "    stats[\"val\"] = val_loss_all\n",
    "    print('epoch', epoch,'; val loss', val_loss_all)\n",
    "\n",
    "    history.append(stats)\n",
    "    if epoch % LR_DECAY_STEP == 0:\n",
    "      for param_group in optimizer.param_groups:\n",
    "          param_group['lr'] = param_group['lr'] / LR_DECAY_VALUE\n",
    "    \n",
    "    if val_loss_all < best_loss:\n",
    "        print('loss improvement on epoch: %d' % (epoch))\n",
    "        best_loss = val_loss_all\n",
    "        best_weights = copy.deepcopy(model.state_dict())\n",
    "        no_improvements = 0\n",
    "    else:\n",
    "        no_improvements += 1\n",
    "    \n",
    "    if no_improvements >= patience:\n",
    "        print('early stopping after epoch {epoch:03d}'.format(**stats))\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "name = f\"gnn_genres_mean_unbiased_popularity_81m_EPOCHS_{EPOCHS}_LR_{LR}_BATCH_SIZE_{BATCH_SIZE}_LR_DECAY_STEP_{LR_DECAY_STEP}_LR_DECAY_VALUE_{LR_DECAY_VALUE}\"\n",
    "torch.save(best_weights, f\"models/mgr/{name}.pt\")\n",
    "header = ['epoch', 'total', 'train', 'val']\n",
    "rows = []\n",
    "\n",
    "with open(f'models/mgr/{name}.csv', 'w', encoding='UTF8', newline='') as f:\n",
    "    writer = csv.DictWriter(f, fieldnames=header)\n",
    "    writer.writeheader()\n",
    "    writer.writerows(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['(no genres listed)' 'Action' 'Adventure' 'Animation' 'Children' 'Comedy'\n",
      " 'Crime' 'Documentary' 'Drama' 'Fantasy' 'Film-Noir' 'Horror' 'IMAX'\n",
      " 'Musical' 'Mystery' 'Romance' 'Sci-Fi' 'Thriller' 'War' 'Western']\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Error(s) in loading state_dict for IGMC:\n\tsize mismatch for linear_layer1.weight: copying a param with shape torch.Size([128, 277]) from checkpoint, the shape in current model is torch.Size([128, 256]).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[129], line 21\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     20\u001b[0m     model \u001b[39m=\u001b[39m model_gnn\u001b[39m.\u001b[39mIGMC()\n\u001b[0;32m---> 21\u001b[0m model\u001b[39m.\u001b[39;49mload_state_dict(torch\u001b[39m.\u001b[39;49mload(path))\n\u001b[1;32m     22\u001b[0m model\u001b[39m.\u001b[39meval()\n\u001b[1;32m     23\u001b[0m model\n",
      "File \u001b[0;32m~/Documents/Nauka/Studia/Magisterskie/Magisterka/Recommender/.venv/lib/python3.9/site-packages/torch/nn/modules/module.py:1671\u001b[0m, in \u001b[0;36mModule.load_state_dict\u001b[0;34m(self, state_dict, strict)\u001b[0m\n\u001b[1;32m   1666\u001b[0m         error_msgs\u001b[39m.\u001b[39minsert(\n\u001b[1;32m   1667\u001b[0m             \u001b[39m0\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mMissing key(s) in state_dict: \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m. \u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mformat(\n\u001b[1;32m   1668\u001b[0m                 \u001b[39m'\u001b[39m\u001b[39m, \u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mjoin(\u001b[39m'\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mformat(k) \u001b[39mfor\u001b[39;00m k \u001b[39min\u001b[39;00m missing_keys)))\n\u001b[1;32m   1670\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(error_msgs) \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m-> 1671\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\u001b[39m'\u001b[39m\u001b[39mError(s) in loading state_dict for \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m:\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\\t\u001b[39;00m\u001b[39m{}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mformat(\n\u001b[1;32m   1672\u001b[0m                        \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\\t\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mjoin(error_msgs)))\n\u001b[1;32m   1673\u001b[0m \u001b[39mreturn\u001b[39;00m _IncompatibleKeys(missing_keys, unexpected_keys)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Error(s) in loading state_dict for IGMC:\n\tsize mismatch for linear_layer1.weight: copying a param with shape torch.Size([128, 277]) from checkpoint, the shape in current model is torch.Size([128, 256])."
     ]
    }
   ],
   "source": [
    "use_features = False\n",
    "movie_df, movie_feature_headers, num_feature_headers = recommender_utils.get_movies_data(filepath='data_small/movies.csv', separator=r',', movies_columns_to_drop=['genres'], genres=True, other_features=None)\n",
    "print(movie_feature_headers)\n",
    "data_test = recommender_utils.get_ratings_data(filepath='data_small/test.csv', separator=r',', dtypes=recommender_utils.dtypes)\n",
    "data_array_test = np.array(data_test.values.tolist())\n",
    "test_adjacency_mx, test_labels, test_user_idx, test_item_idx, test_item_dict = recommender_utils.preprocess_data_to_graph(data_array_test, dtypes=recommender_utils.dtypes, class_values=recommender_utils.class_values)\n",
    "test_item_features = sp.csr_matrix(recommender_utils.get_movies_features(movie_df, test_item_dict, movie_feature_headers, num_feature_headers))\n",
    "test_user_features = sp.csr_matrix(recommender_utils.get_user_features(test_user_idx))\n",
    "test_item_features_array = test_item_features.toarray() if use_features else None\n",
    "test_user_features_array = test_user_features.toarray() if use_features else None\n",
    "test_dataset = MyDynamicDataset(root='data_test/processed/test', A=test_adjacency_mx, \n",
    "    links=(test_user_idx, test_item_idx), labels=test_labels, h=1, sample_ratio=1.0, \n",
    "    max_nodes_per_hop=200, u_features=test_user_features_array, v_features=test_item_features_array, class_values=recommender_utils.class_values)\n",
    "test_loader = DataLoader(test_dataset, BATCH_SIZE, shuffle=False, num_workers=2)\n",
    "pre_path=\"/Users/user/Documents/Nauka/Studia/Magisterskie/Magisterka/Recommender/models/mgr/\"\n",
    "path=f\"{pre_path}gnn_no_features_90m_EPOCHS_80_LR_0.001_BATCH_SIZE_50_LR_DECAY_STEP_20_LR_DECAY_VALUE_10.pt\"\n",
    "if use_features:\n",
    "    model = model_gnn.IGMC(side_features=True, n_side_features=test_item_features_array.shape[1])\n",
    "else:\n",
    "    model = model_gnn.IGMC()\n",
    "model.load_state_dict(torch.load(path))\n",
    "model.eval()\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0/302\n",
      "1/302\n",
      "2/302\n",
      "3/302\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/user/Documents/Nauka/Studia/Magisterskie/Magisterka/Recommender/.venv/lib/python3.9/site-packages/torch_geometric/deprecation.py:12: UserWarning: 'dropout_adj' is deprecated, use 'dropout_edge' instead\n",
      "  warnings.warn(out)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4/302\n",
      "5/302\n",
      "6/302\n",
      "7/302\n",
      "8/302\n",
      "9/302\n",
      "10/302\n",
      "11/302\n",
      "12/302\n",
      "13/302\n",
      "14/302\n",
      "15/302\n",
      "16/302\n",
      "17/302\n",
      "18/302\n",
      "19/302\n",
      "20/302\n",
      "21/302\n",
      "22/302\n",
      "23/302\n",
      "24/302\n",
      "25/302\n",
      "26/302\n",
      "27/302\n",
      "28/302\n",
      "29/302\n",
      "30/302\n",
      "31/302\n",
      "32/302\n",
      "33/302\n",
      "34/302\n",
      "35/302\n",
      "36/302\n",
      "37/302\n",
      "38/302\n",
      "39/302\n",
      "40/302\n",
      "41/302\n",
      "42/302\n",
      "43/302\n",
      "44/302\n",
      "45/302\n",
      "46/302\n",
      "47/302\n",
      "48/302\n",
      "49/302\n",
      "50/302\n",
      "51/302\n",
      "52/302\n",
      "53/302\n",
      "54/302\n",
      "55/302\n",
      "56/302\n",
      "57/302\n",
      "58/302\n",
      "59/302\n",
      "60/302\n",
      "61/302\n",
      "62/302\n",
      "63/302\n",
      "64/302\n",
      "65/302\n",
      "66/302\n",
      "67/302\n",
      "68/302\n",
      "69/302\n",
      "70/302\n",
      "71/302\n",
      "72/302\n",
      "73/302\n",
      "74/302\n",
      "75/302\n",
      "76/302\n",
      "77/302\n",
      "78/302\n",
      "79/302\n",
      "80/302\n",
      "81/302\n",
      "82/302\n",
      "83/302\n",
      "84/302\n",
      "85/302\n",
      "86/302\n",
      "87/302\n",
      "88/302\n",
      "89/302\n",
      "90/302\n",
      "91/302\n",
      "92/302\n",
      "93/302\n",
      "94/302\n",
      "95/302\n",
      "96/302\n",
      "97/302\n",
      "98/302\n",
      "99/302\n",
      "100/302\n",
      "101/302\n",
      "102/302\n",
      "103/302\n",
      "104/302\n",
      "105/302\n",
      "106/302\n",
      "107/302\n",
      "108/302\n",
      "109/302\n",
      "110/302\n",
      "111/302\n",
      "112/302\n",
      "113/302\n",
      "114/302\n",
      "115/302\n",
      "116/302\n",
      "117/302\n",
      "118/302\n",
      "119/302\n",
      "120/302\n",
      "121/302\n",
      "122/302\n",
      "123/302\n",
      "124/302\n",
      "125/302\n",
      "126/302\n",
      "127/302\n",
      "128/302\n",
      "129/302\n",
      "130/302\n",
      "131/302\n",
      "132/302\n",
      "133/302\n",
      "134/302\n",
      "135/302\n",
      "136/302\n",
      "137/302\n",
      "138/302\n",
      "139/302\n",
      "140/302\n",
      "141/302\n",
      "142/302\n",
      "143/302\n",
      "144/302\n",
      "145/302\n",
      "146/302\n",
      "147/302\n",
      "148/302\n",
      "149/302\n",
      "150/302\n",
      "151/302\n",
      "152/302\n",
      "153/302\n",
      "154/302\n",
      "155/302\n",
      "156/302\n",
      "157/302\n",
      "158/302\n",
      "159/302\n",
      "160/302\n",
      "161/302\n",
      "162/302\n",
      "163/302\n",
      "164/302\n",
      "165/302\n",
      "166/302\n",
      "167/302\n",
      "168/302\n",
      "169/302\n",
      "170/302\n",
      "171/302\n",
      "172/302\n",
      "173/302\n",
      "174/302\n",
      "175/302\n",
      "176/302\n",
      "177/302\n",
      "178/302\n",
      "179/302\n",
      "180/302\n",
      "181/302\n",
      "182/302\n",
      "183/302\n",
      "184/302\n",
      "185/302\n",
      "186/302\n",
      "187/302\n",
      "188/302\n",
      "189/302\n",
      "190/302\n",
      "191/302\n",
      "192/302\n",
      "193/302\n",
      "194/302\n",
      "195/302\n",
      "196/302\n",
      "197/302\n",
      "198/302\n",
      "199/302\n",
      "200/302\n",
      "201/302\n",
      "202/302\n",
      "203/302\n",
      "204/302\n",
      "205/302\n",
      "206/302\n",
      "207/302\n",
      "208/302\n",
      "209/302\n",
      "210/302\n",
      "211/302\n",
      "212/302\n",
      "213/302\n",
      "214/302\n",
      "215/302\n",
      "216/302\n",
      "217/302\n",
      "218/302\n",
      "219/302\n",
      "220/302\n",
      "221/302\n",
      "222/302\n",
      "223/302\n",
      "224/302\n",
      "225/302\n",
      "226/302\n",
      "227/302\n",
      "228/302\n",
      "229/302\n",
      "230/302\n",
      "231/302\n",
      "232/302\n",
      "233/302\n",
      "234/302\n",
      "235/302\n",
      "236/302\n",
      "237/302\n",
      "238/302\n",
      "239/302\n",
      "240/302\n",
      "241/302\n",
      "242/302\n",
      "243/302\n",
      "244/302\n",
      "245/302\n",
      "246/302\n",
      "247/302\n",
      "248/302\n",
      "249/302\n",
      "250/302\n",
      "251/302\n",
      "252/302\n",
      "253/302\n",
      "254/302\n",
      "255/302\n",
      "256/302\n",
      "257/302\n",
      "258/302\n",
      "259/302\n",
      "260/302\n",
      "261/302\n",
      "262/302\n",
      "263/302\n",
      "264/302\n",
      "265/302\n",
      "266/302\n",
      "267/302\n",
      "268/302\n",
      "269/302\n",
      "270/302\n",
      "271/302\n",
      "272/302\n",
      "273/302\n",
      "274/302\n",
      "275/302\n",
      "276/302\n",
      "277/302\n",
      "278/302\n",
      "279/302\n",
      "280/302\n",
      "281/302\n",
      "282/302\n",
      "283/302\n",
      "284/302\n",
      "285/302\n",
      "286/302\n",
      "287/302\n",
      "288/302\n",
      "289/302\n",
      "290/302\n",
      "291/302\n",
      "292/302\n",
      "293/302\n",
      "294/302\n",
      "295/302\n",
      "296/302\n",
      "297/302\n",
      "298/302\n",
      "299/302\n",
      "300/302\n",
      "301/302\n",
      "test MSE loss 0.915024343710577\n",
      "test RMSE loss 0.9565690480621757\n"
     ]
    }
   ],
   "source": [
    "batches_in_eval = len(test_loader)\n",
    "model.eval()\n",
    "test_loss = 0\n",
    "for i, test_batch in enumerate(test_loader):\n",
    "    print(f\"{i}/{batches_in_eval}\")\n",
    "    test_batch = test_batch.to(device)\n",
    "    with torch.no_grad():\n",
    "        y_pred = model(test_batch)\n",
    "    y_true = test_batch.y\n",
    "    test_loss += F.mse_loss(y_pred, y_true, reduction='sum')\n",
    "    torch.cuda.empty_cache()\n",
    "mse_loss = float(test_loss) / len(test_loader.dataset)\n",
    "\n",
    "print('test MSE loss', mse_loss)\n",
    "print('test RMSE loss', math.sqrt(mse_loss))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "f24bbe3eee7f1671aa96cb4424e1ae784be864432abf663e2856784d4fd29d28"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
