{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import recommender_utils\n",
    "import math\n",
    "import copy\n",
    "\n",
    "import model_CNN as model_cnn\n",
    "import torch\n",
    "import numpy as np\n",
    "from pandas import concat\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset treningowy:\n",
      "\tEmbeddings: 427 uzytkowników, 8500 filmów\n",
      "\tX wymiar: (68495, 2)\n",
      "\tY shape: (68495,)\n",
      "426\n",
      "8499\n",
      "Dataset walidacyjny:\n",
      "\tEmbeddings: 61 uzytkowników, 5536 filmów\n",
      "\tX wymiar: (17280, 2)\n",
      "\tY shape: (17280,)\n",
      "487\n",
      "9448\n",
      "Dataset testowy:\n",
      "\tEmbeddings: 122 uzytkowników, 3975 filmów\n",
      "\tX wymiar: (15061, 2)\n",
      "\tY shape: (15061,)\n",
      "609\n",
      "9723\n"
     ]
    }
   ],
   "source": [
    "movie_df, movie_feature_headers, num_feature_headers = recommender_utils.get_movies_data(filepath='data_small/movies.csv', separator=r',', movies_columns_to_drop=['genres'], only_genres=False)\n",
    "\n",
    "data_train = recommender_utils.get_ratings_data(filepath='data_small/train.csv', separator=r',', dtypes=recommender_utils.dtypes)\n",
    "data_val = recommender_utils.get_ratings_data(filepath='data_small/validate.csv', separator=r',', dtypes=recommender_utils.dtypes)\n",
    "data_test = recommender_utils.get_ratings_data(filepath='data_small/test.csv', separator=r',', dtypes=recommender_utils.dtypes)\n",
    "\n",
    "user_to_index = recommender_utils.map_to_new_indexes(concat([data_train, data_val, data_test]), column='user')\n",
    "movie_to_index = recommender_utils.map_to_new_indexes(concat([data_train, data_val, data_test]), column='item')\n",
    "\n",
    "(num_users_train, num_movies_train), (X_train, y_train) = recommender_utils.create_dataset_cnn(data_train, user_to_index, movie_to_index)\n",
    "(num_users_val, num_movies_val), (X_val, y_val) = recommender_utils.create_dataset_cnn(data_val, user_to_index, movie_to_index)\n",
    "(num_users_test, num_movies_test), (X_test, y_test) = recommender_utils.create_dataset_cnn(data_test, user_to_index, movie_to_index)\n",
    "\n",
    "\n",
    "print(\"Dataset treningowy:\")\n",
    "print(f\"\\tEmbeddings: {num_users_train} uzytkowników, {num_movies_train} filmów\")\n",
    "print(f\"\\tX wymiar: {X_train.shape}\")\n",
    "print(f\"\\tY shape: {y_train.shape}\")\n",
    "print(X_train.user_id.max())\n",
    "print(X_train.movie_id.max())\n",
    "\n",
    "print(\"Dataset walidacyjny:\")\n",
    "print(f\"\\tEmbeddings: {num_users_val} uzytkowników, {num_movies_val} filmów\")\n",
    "print(f\"\\tX wymiar: {X_val.shape}\")\n",
    "print(f\"\\tY shape: {y_val.shape}\")\n",
    "print(X_val.user_id.max())\n",
    "print(X_val.movie_id.max())\n",
    "\n",
    "print(\"Dataset testowy:\")\n",
    "print(f\"\\tEmbeddings: {num_users_test} uzytkowników, {num_movies_test} filmów\")\n",
    "print(f\"\\tX wymiar: {X_test.shape}\")\n",
    "print(f\"\\tY shape: {y_test.shape}\")\n",
    "print(X_test.user_id.max())\n",
    "print(X_test.movie_id.max())\n",
    "\n",
    "datasets = {'train': (X_train, y_train), 'val': (X_val, y_val)}\n",
    "dataset_sizes = {'train': len(X_train), 'val': len(X_val)}\n",
    "\n",
    "minmax = y_train.min().astype(float), y_train.max().astype(float)\n",
    "\n",
    "num_users_all = num_users_train + num_users_val + num_users_test\n",
    "num_movies_all = num_movies_train + num_movies_val + num_movies_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model_cnn.ConvEmbeddingNet(\n",
    "    n_users=num_users_all, n_movies=num_movies_all, \n",
    "    n_factors=1000, hidden=[500, 500, 500], \n",
    "    embedding_dropout=0.05, dropouts=[0.5, 0.5, 0.25])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ConvEmbeddingNet(\n",
       "  (u): Embedding(610, 1000)\n",
       "  (m): Embedding(18011, 1000)\n",
       "  (drop): Dropout(p=0.05, inplace=False)\n",
       "  (hidden): Sequential(\n",
       "    (0): Conv1d(2000, 500, kernel_size=(5,), stride=(1,), padding=(2,), bias=False)\n",
       "    (1): ReLU()\n",
       "    (2): Dropout(p=0.5, inplace=False)\n",
       "    (3): Conv1d(500, 500, kernel_size=(5,), stride=(1,), padding=(2,), bias=False)\n",
       "    (4): ReLU()\n",
       "    (5): Dropout(p=0.5, inplace=False)\n",
       "    (6): Conv1d(500, 500, kernel_size=(5,), stride=(1,), padding=(2,), bias=False)\n",
       "    (7): ReLU()\n",
       "    (8): Dropout(p=0.25, inplace=False)\n",
       "  )\n",
       "  (fc): Linear(in_features=500, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 203,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss improvement on epoch: 1\n",
      "[001/1000] train: 1.5817 - val: 1.2020\n",
      "loss improvement on epoch: 2\n",
      "[002/1000] train: 1.3583 - val: 0.9620\n",
      "loss improvement on epoch: 3\n",
      "[003/1000] train: 1.1427 - val: 0.8972\n",
      "loss improvement on epoch: 4\n",
      "[004/1000] train: 1.1181 - val: 0.8943\n",
      "[005/1000] train: 1.1173 - val: 0.8949\n",
      "[006/1000] train: 1.1183 - val: 0.8947\n",
      "loss improvement on epoch: 7\n",
      "[007/1000] train: 1.1178 - val: 0.8925\n",
      "loss improvement on epoch: 8\n",
      "[008/1000] train: 1.1180 - val: 0.8921\n",
      "[009/1000] train: 1.1173 - val: 0.8927\n",
      "loss improvement on epoch: 10\n",
      "[010/1000] train: 1.1177 - val: 0.8914\n",
      "[011/1000] train: 1.1174 - val: 0.8915\n",
      "loss improvement on epoch: 12\n",
      "[012/1000] train: 1.1172 - val: 0.8907\n",
      "loss improvement on epoch: 13\n",
      "[013/1000] train: 1.1183 - val: 0.8891\n",
      "[014/1000] train: 1.1182 - val: 0.8908\n",
      "[015/1000] train: 1.1186 - val: 0.8902\n",
      "[016/1000] train: 1.1180 - val: 0.8896\n",
      "loss improvement on epoch: 17\n",
      "[017/1000] train: 1.1169 - val: 0.8884\n",
      "[018/1000] train: 1.1174 - val: 0.8887\n",
      "loss improvement on epoch: 19\n",
      "[019/1000] train: 1.1176 - val: 0.8882\n",
      "loss improvement on epoch: 20\n",
      "[020/1000] train: 1.1181 - val: 0.8879\n",
      "loss improvement on epoch: 21\n",
      "[021/1000] train: 1.1186 - val: 0.8875\n",
      "[022/1000] train: 1.1183 - val: 0.8892\n",
      "[023/1000] train: 1.1178 - val: 0.8885\n",
      "[024/1000] train: 1.1169 - val: 0.8883\n",
      "[025/1000] train: 1.1189 - val: 0.8889\n",
      "[026/1000] train: 1.1177 - val: 0.8891\n",
      "[027/1000] train: 1.1182 - val: 0.8900\n",
      "[028/1000] train: 1.1183 - val: 0.8896\n",
      "[029/1000] train: 1.1169 - val: 0.8901\n",
      "[030/1000] train: 1.1188 - val: 0.8902\n",
      "[031/1000] train: 1.1171 - val: 0.8902\n",
      "early stopping after epoch 031\n"
     ]
    }
   ],
   "source": [
    "RANDOM_STATE = 1\n",
    "torch.manual_seed(RANDOM_STATE)\n",
    "torch.cuda.manual_seed(RANDOM_STATE)\n",
    "lr = 1e-5\n",
    "wd = 1e-4\n",
    "bs = 2000\n",
    "n_epochs = 1000\n",
    "patience = 10\n",
    "no_improvements = 0\n",
    "best_loss = np.inf\n",
    "best_weights = None\n",
    "use_scheduler = False\n",
    "history = []\n",
    "lr_history = []\n",
    "\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "model.to(device)\n",
    "criterion = torch.nn.MSELoss(reduction='sum')\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=wd)\n",
    "iterations_per_epoch = int(math.ceil(dataset_sizes['train'] // bs))\n",
    "if use_scheduler:\n",
    "    scheduler = model_cnn.CyclicLR(optimizer, model_cnn.cosine(t_max=iterations_per_epoch * 2, eta_min=lr/10))\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    stats = {'epoch': epoch + 1, 'total': n_epochs}\n",
    "    \n",
    "    for phase in ('train', 'val'):\n",
    "        training = phase == 'train'\n",
    "        running_loss = 0.0\n",
    "        n_batches = 0\n",
    "        \n",
    "        for batch in model_cnn.batches(*datasets[phase], shuffle=training, bs=bs):\n",
    "            x_batch, y_batch = [b.to(device) for b in batch]\n",
    "            optimizer.zero_grad()\n",
    "        \n",
    "            # compute gradients only during 'train' phase\n",
    "            with torch.set_grad_enabled(training):\n",
    "                outputs = model(x_batch[:, 0], x_batch[:, 1], minmax)\n",
    "                loss = criterion(outputs, y_batch)\n",
    "                \n",
    "                # don't update weights and rates when in 'val' phase\n",
    "                if training:\n",
    "                    if use_scheduler:\n",
    "                        scheduler.step()\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "                    if use_scheduler:\n",
    "                        lr_history.extend(scheduler.get_lr())\n",
    "                    \n",
    "            running_loss += loss.item()\n",
    "            \n",
    "        epoch_loss = running_loss / dataset_sizes[phase]\n",
    "        stats[phase] = epoch_loss\n",
    "        \n",
    "        # early stopping: save weights of the best model so far\n",
    "        if phase == 'val':\n",
    "            if epoch_loss < best_loss:\n",
    "                print('loss improvement on epoch: %d' % (epoch + 1))\n",
    "                best_loss = epoch_loss\n",
    "                best_weights = copy.deepcopy(model.state_dict())\n",
    "                no_improvements = 0\n",
    "            else:\n",
    "                no_improvements += 1\n",
    "                \n",
    "    history.append(stats)\n",
    "    print('[{epoch:03d}/{total:03d}] train: {train:.4f} - val: {val:.4f}'.format(**stats))\n",
    "    if no_improvements >= patience:\n",
    "        print('early stopping after epoch {epoch:03d}'.format(**stats))\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ConvEmbeddingNet(\n",
       "  (u): Embedding(610, 1000)\n",
       "  (m): Embedding(18011, 1000)\n",
       "  (drop): Dropout(p=0.05, inplace=False)\n",
       "  (hidden): Sequential(\n",
       "    (0): Conv1d(2000, 500, kernel_size=(5,), stride=(1,), padding=(2,), bias=False)\n",
       "    (1): ReLU()\n",
       "    (2): Dropout(p=0.5, inplace=False)\n",
       "    (3): Conv1d(500, 500, kernel_size=(5,), stride=(1,), padding=(2,), bias=False)\n",
       "    (4): ReLU()\n",
       "    (5): Dropout(p=0.5, inplace=False)\n",
       "    (6): Conv1d(500, 500, kernel_size=(5,), stride=(1,), padding=(2,), bias=False)\n",
       "    (7): ReLU()\n",
       "    (8): Dropout(p=0.25, inplace=False)\n",
       "  )\n",
       "  (fc): Linear(in_features=500, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 205,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "name = \"cnn_100epochs_wo_features_100b_50f_5ker_2pad_wd1e-4_lr1e-5\"\n",
    "torch.save(best_weights, f\"models/mgr/{name}.pt\")\n",
    "header = ['epoch', 'total', 'train', 'val']\n",
    "rows = []\n",
    "\n",
    "with open(f'models/mgr/{name}.csv', 'w', encoding='UTF8', newline='') as f:\n",
    "    writer = csv.DictWriter(f, fieldnames=header)\n",
    "    writer.writeheader()\n",
    "    writer.writerows(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'epoch': 1, 'total': 100, 'train': 1.6522281200348508, 'val': 1.4264369673199124}, {'epoch': 2, 'total': 100, 'train': 1.6346701385522553, 'val': 1.4094229998411956}, {'epoch': 3, 'total': 100, 'train': 1.615244218417089, 'val': 1.3908839631963659}, {'epoch': 4, 'total': 100, 'train': 1.5942582345198555, 'val': 1.3702423713825367}, {'epoch': 5, 'total': 100, 'train': 1.5701569643862292, 'val': 1.3472909609476724}, {'epoch': 6, 'total': 100, 'train': 1.543974967949303, 'val': 1.320873851246304}, {'epoch': 7, 'total': 100, 'train': 1.5132552462343283, 'val': 1.2922321725774695}, {'epoch': 8, 'total': 100, 'train': 1.48048256010076, 'val': 1.2610000389593619}, {'epoch': 9, 'total': 100, 'train': 1.444400892557737, 'val': 1.228015998557762}, {'epoch': 10, 'total': 100, 'train': 1.40622381667985, 'val': 1.1942142371778135}, {'epoch': 11, 'total': 100, 'train': 1.3691254346020352, 'val': 1.1604599802582352}, {'epoch': 12, 'total': 100, 'train': 1.3303933357825217, 'val': 1.128865404482241}, {'epoch': 13, 'total': 100, 'train': 1.295972307014312, 'val': 1.099177402920193}, {'epoch': 14, 'total': 100, 'train': 1.262067378856064, 'val': 1.073835055033366}, {'epoch': 15, 'total': 100, 'train': 1.2330386584445032, 'val': 1.050338833420365}, {'epoch': 16, 'total': 100, 'train': 1.2069082231032204, 'val': 1.033367094287166}, {'epoch': 17, 'total': 100, 'train': 1.1856858345720662, 'val': 1.0160341148023253}, {'epoch': 18, 'total': 100, 'train': 1.1692810771465127, 'val': 1.0074279374546475}, {'epoch': 19, 'total': 100, 'train': 1.1562853806975533, 'val': 0.9997252137572677}, {'epoch': 20, 'total': 100, 'train': 1.14695490664379, 'val': 0.9963066705950985}, {'epoch': 21, 'total': 100, 'train': 1.1409906998978312, 'val': 0.9922950214809841}, {'epoch': 22, 'total': 100, 'train': 1.1358143367665694, 'val': 0.9933453427420722}, {'epoch': 23, 'total': 100, 'train': 1.134917395316264, 'val': 0.9929364094027766}, {'epoch': 24, 'total': 100, 'train': 1.1305234040663323, 'val': 0.9922095585752416}, {'epoch': 25, 'total': 100, 'train': 1.1303086885440714, 'val': 0.991305304898156}, {'epoch': 26, 'total': 100, 'train': 1.1304734825748572, 'val': 0.9939768208397759}, {'epoch': 27, 'total': 100, 'train': 1.1300255275675917, 'val': 0.9924713019971494}, {'epoch': 28, 'total': 100, 'train': 1.130182506745902, 'val': 0.9950697669276485}, {'epoch': 29, 'total': 100, 'train': 1.1317629777877833, 'val': 0.9929046970826608}, {'epoch': 30, 'total': 100, 'train': 1.1296034212764732, 'val': 0.9927924721329301}, {'epoch': 31, 'total': 100, 'train': 1.1310636736892576, 'val': 0.9941163045388681}, {'epoch': 32, 'total': 100, 'train': 1.1306109714041628, 'val': 0.994728970086133}, {'epoch': 33, 'total': 100, 'train': 1.1285761958987655, 'val': 0.9944509916835361}, {'epoch': 34, 'total': 100, 'train': 1.1304361238820038, 'val': 0.9943775199077748}, {'epoch': 35, 'total': 100, 'train': 1.1304172748944665, 'val': 0.994283409471865}]\n",
      "{'epoch': 35, 'total': 100, 'train': 1.1304172748944665, 'val': 0.994283409471865}\n"
     ]
    }
   ],
   "source": [
    "print(history)\n",
    "print(stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 1.0153876271003128\n"
     ]
    }
   ],
   "source": [
    "running_loss = 0.0\n",
    "n_batches = 0\n",
    "\n",
    "for batch in model_cnn.batches(X_test, y_test, shuffle=False, bs=bs):\n",
    "    x_batch, y_batch = [b.to(device) for b in batch]\n",
    "    with torch.set_grad_enabled(False):\n",
    "        outputs = model(x_batch[:, 0], x_batch[:, 1], minmax)\n",
    "        loss = criterion(outputs, y_batch)\n",
    "    running_loss += loss.item()\n",
    "    \n",
    "test_loss = running_loss / len(X_test)\n",
    "print(f\"Test loss: {test_loss}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "f24bbe3eee7f1671aa96cb4424e1ae784be864432abf663e2856784d4fd29d28"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
