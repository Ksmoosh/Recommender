{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.sparse as sp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_data(data):\n",
    "    \"\"\"\n",
    "    From IGMC data_utils.py\n",
    "    Map data to proper indices in case they are not in a continues [0, N) range\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    data : np.int32 arrays\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    mapped_data : np.int32 arrays\n",
    "    n : length of mapped_data\n",
    "\n",
    "    \"\"\"\n",
    "    uniq = list(set(data))\n",
    "\n",
    "    id_dict = {old: new for new, old in enumerate(sorted(uniq))}\n",
    "    data = np.array([id_dict[x] for x in data])\n",
    "    n = len(uniq)\n",
    "\n",
    "    return data, id_dict, n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_movies_data(filepath=None, separator=None, movies_columns_to_drop=None):\n",
    "    movie_headers = ['item', 'title', 'genres', 'mean', 'popularity', 'mean_unbiased', \n",
    "                    '(no genres listed)', 'Action', 'Adventure', 'Animation', 'Children',\n",
    "                    'Comedy', 'Crime', 'Documentary', 'Drama', 'Fantasy', 'Film-Noir',\n",
    "                    'Horror', 'IMAX', 'Musical', 'Mystery', 'Romance', 'Sci-Fi', 'Thriller',\n",
    "                    'War', 'Western']\n",
    "\n",
    "    movie_df = pd.read_csv(filepath, sep=separator, header=None,\n",
    "                           names=movie_headers, engine='python', encoding='ISO-8859-1', skiprows=1)\n",
    "    for column_to_drop in movies_columns_to_drop:\n",
    "        if column_to_drop in movie_df.columns:\n",
    "            movie_df.drop(column_to_drop, axis=1, inplace=True)\n",
    "    genre_headers = movie_df.columns.values[7:]\n",
    "    num_genres = genre_headers.shape[0]\n",
    "    return movie_df, genre_headers, num_genres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ratings_data(filepath=None, separator=None, dtypes=None):\n",
    "    return pd.read_csv(\n",
    "        filepath, sep=separator, header=None,\n",
    "        names=[\"user\", \"item\", \"rating\", \"timestamp\"], dtype=dtypes, skiprows=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data_to_graph(data_array, testing=False, rating_map=None, post_rating_map=None, ratio=1.0, dtypes=None, class_values=None):\n",
    "    \"\"\"\n",
    "    Loads official train/test split and uses 10% of training samples for validaiton\n",
    "    For each split computes 1-of-num_classes labels. Also computes training\n",
    "    adjacency matrix. Assumes flattening happens everywhere in row-major fashion.\n",
    "    \"\"\"\n",
    "    if ratio < 1.0:\n",
    "        data_array = data_array[data_array[:, -1].argsort()[:int(ratio*len(data_array))]]\n",
    "\n",
    "    user_nodes_ratings = data_array[:, 0].astype(dtypes['user'])\n",
    "    item_nodes_ratings = data_array[:, 1].astype(dtypes['item'])\n",
    "    ratings = data_array[:, 2].astype(dtypes['rating'])\n",
    "    if rating_map is not None:\n",
    "        for i, x in enumerate(ratings):\n",
    "            ratings[i] = rating_map[x]\n",
    "\n",
    "    user_nodes_ratings, user_dict, num_users = map_data(user_nodes_ratings)\n",
    "    item_nodes_ratings, item_dict, num_items = map_data(item_nodes_ratings)\n",
    "\n",
    "    user_nodes_ratings, item_nodes_ratings, ratings = user_nodes_ratings.astype(np.int64), item_nodes_ratings.astype(np.int32), ratings.astype(np.float64)\n",
    "\n",
    "    neutral_rating = -1  # int(np.ceil(np.float(num_classes)/2.)) - 1\n",
    "\n",
    "    # assumes that ratings_train contains at least one example of every rating type\n",
    "    rating_dict = {r: i for i, r in enumerate(class_values.tolist())}\n",
    "\n",
    "    labels = np.full((num_users, num_items), neutral_rating, dtype=np.int32)\n",
    "    labels[user_nodes_ratings, item_nodes_ratings] = np.array([rating_dict[r] for r in ratings])\n",
    "\n",
    "    for i in range(len(user_nodes_ratings)):\n",
    "        assert(labels[user_nodes_ratings[i], item_nodes_ratings[i]] == rating_dict[ratings[i]])\n",
    "\n",
    "    labels = labels.reshape([-1])\n",
    "\n",
    "    # number of test and validation edges, see cf-nade code\n",
    "\n",
    "    num_edges = data_array.shape[0]\n",
    "\n",
    "    pairs_nonzero = np.array([[u, v] for u, v in zip(user_nodes_ratings, item_nodes_ratings)])\n",
    "    idx_nonzero = np.array([u * num_items + v for u, v in pairs_nonzero])\n",
    "\n",
    "    for i in range(len(ratings)):\n",
    "        assert(labels[idx_nonzero[i]] == rating_dict[ratings[i]])\n",
    "\n",
    "    assert(len(idx_nonzero) == num_edges)\n",
    "\n",
    "    user_idx, item_idx = pairs_nonzero.transpose()\n",
    "\n",
    "    # create labels\n",
    "    nonzero_labels = labels[idx_nonzero]\n",
    "\n",
    "    # make training adjacency matrix\n",
    "    rating_mx_train = np.zeros(num_users * num_items, dtype=np.float32)\n",
    "    if post_rating_map is None:\n",
    "        rating_mx_train[idx_nonzero] = labels[idx_nonzero].astype(np.float32) + 1.\n",
    "    else:\n",
    "        rating_mx_train[idx_nonzero] = np.array([post_rating_map[r] for r in class_values[labels[idx_nonzero]]]) + 1.\n",
    "    rating_mx_train = sp.csr_matrix(rating_mx_train.reshape(num_users, num_items))\n",
    "\n",
    "    return rating_mx_train, nonzero_labels, user_idx, item_idx, item_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_movies_features(movie_df, item_dict, genre_headers, num_genres):\n",
    "    item_features = np.zeros((len(item_dict.keys()), num_genres), dtype=np.float32)\n",
    "    for movie_id, g_vec in zip(movie_df['item'].values.tolist(), movie_df[genre_headers].values.tolist()):\n",
    "        # check if movie_id was listed in ratings file and therefore in mapping dictionary\n",
    "        if movie_id in item_dict.keys():\n",
    "            item_features[item_dict[movie_id], :] = g_vec\n",
    "    return item_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x11cacd790>\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/user/Documents/Nauka/Studia/Magisterskie/Magisterka/Recommender/.venv/lib/python3.9/site-packages/torch/utils/data/dataloader.py\", line 1466, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/Users/user/Documents/Nauka/Studia/Magisterskie/Magisterka/Recommender/.venv/lib/python3.9/site-packages/torch/utils/data/dataloader.py\", line 1424, in _shutdown_workers\n",
      "    if self._persistent_workers or self._workers_status[worker_id]:\n",
      "AttributeError: '_MultiProcessingDataLoaderIter' object has no attribute '_workers_status'\n",
      "Traceback (most recent call last):\n",
      "  File \"<string>\", line 1, in <module>\n",
      "  File \"/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/multiprocessing/spawn.py\", line 116, in spawn_main\n",
      "    exitcode = _main(fd, parent_sentinel)\n",
      "  File \"/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/multiprocessing/spawn.py\", line 126, in _main\n",
      "    self = reduction.pickle.load(from_parent)\n",
      "_pickle.UnpicklingError: pickle data was truncated\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train item features shape: (56232, 18)\n",
      "Validation item features shape: (29750, 18)\n",
      "Test item features shape: (38341, 18)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "movie_df, genre_headers, num_genres = get_movies_data(filepath='data/movies.csv', separator=r',', movies_columns_to_drop=['genres'])\n",
    "\n",
    "dtypes = {\n",
    "    'user': np.int32, 'item': np.int32,\n",
    "    'rating': np.float32, 'timestamp': np.float64}\n",
    "\n",
    "data_train = get_ratings_data(filepath='data/train.csv', separator=r',', dtypes=dtypes)\n",
    "data_val = get_ratings_data(filepath='data/validate.csv', separator=r',', dtypes=dtypes)\n",
    "data_test = get_ratings_data(filepath='data/test.csv', separator=r',', dtypes=dtypes)\n",
    "\n",
    "data_array_train = np.array(data_train.values.tolist())\n",
    "data_array_val = np.array(data_val.values.tolist())\n",
    "data_array_test = np.array(data_test.values.tolist())\n",
    "\n",
    "class_values = np.array([0.5, 1, 1.5, 2, 2.5, 3, 3.5, 4, 4.5, 5], dtype=dtypes['rating'])\n",
    "\n",
    "train_adjacency_mx, train_labels, train_user_idx, train_item_idx, train_item_dict = preprocess_data_to_graph(data_array_train, dtypes=dtypes, class_values=class_values)\n",
    "train_item_features = sp.csr_matrix(get_movies_features(movie_df, train_item_dict, genre_headers, num_genres))\n",
    "val_adjacency_mx, val_labels, val_user_idx, val_item_idx, val_item_dict = preprocess_data_to_graph(data_array_val, dtypes=dtypes, class_values=class_values)\n",
    "val_item_features = sp.csr_matrix(get_movies_features(movie_df, val_item_dict, genre_headers, num_genres))\n",
    "test_adjacency_mx, test_labels, test_user_idx, test_item_idx, test_item_dict = preprocess_data_to_graph(data_array_test, dtypes=dtypes, class_values=class_values)\n",
    "test_item_features = sp.csr_matrix(get_movies_features(movie_df, test_item_dict, genre_headers, num_genres))\n",
    "\n",
    "print(\"Train item features shape: \"+str(train_item_features.shape))\n",
    "print(\"Validation item features shape: \"+str(val_item_features.shape))\n",
    "print(\"Test item features shape: \"+str(test_item_features.shape))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.nn import Linear\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import Adam\n",
    "from torch_geometric.data import DataLoader\n",
    "from torch_geometric.nn import RGCNConv\n",
    "from torch_geometric.utils import dropout_adj\n",
    "from IGMC.util_functions import *\n",
    "from IGMC.data_utils import *\n",
    "from IGMC.preprocessing import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = eval('MyDynamicDataset')(root='data_test/processed/train', A=train_adjacency_mx, \n",
    "    links=(train_user_idx, train_item_idx), labels=train_labels, h=1, sample_ratio=1.0, \n",
    "    max_nodes_per_hop=200, u_features=None, v_features=train_item_features, class_values=class_values)\n",
    "test_dataset = eval('MyDataset')(root='data_test/processed/test', A=test_adjacency_mx, \n",
    "    links=(test_user_idx, test_item_idx), labels=test_labels, h=1, sample_ratio=1.0, \n",
    "    max_nodes_per_hop=200, u_features=None, v_features=test_item_features, class_values=class_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import math\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import Linear\n",
    "from torch_geometric.nn import RGCNConv\n",
    "from torch_geometric.utils import dropout_adj\n",
    "\n",
    "class IGMC(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(IGMC, self).__init__()\n",
    "        self.rel_graph_convs = torch.nn.ModuleList()\n",
    "        self.rel_graph_convs.append(RGCNConv(in_channels=4, out_channels=32,\\\n",
    "                                             num_relations=5, num_bases=4))\n",
    "        self.rel_graph_convs.append(RGCNConv(in_channels=32, out_channels=32\\\n",
    "                                             , num_relations=5, num_bases=4))\n",
    "        self.rel_graph_convs.append(RGCNConv(in_channels=32, out_channels=32,\\\n",
    "                                             num_relations=5, num_bases=4))\n",
    "        self.rel_graph_convs.append(RGCNConv(in_channels=32, out_channels=32,\\\n",
    "                                             num_relations=5, num_bases=4))\n",
    "        self.linear_layer1 = Linear(256, 128)\n",
    "        self.linear_layer2 = Linear(128, 1)\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        self.linear_layer1.reset_parameters()\n",
    "        self.linear_layer2.reset_parameters()\n",
    "        for i in self.rel_graph_convs:\n",
    "            i.reset_parameters()\n",
    "\n",
    "    def forward(self, data):\n",
    "        num_nodes = len(data.x)\n",
    "        edge_index_dr, edge_type_dr = dropout_adj(data.edge_index, data.edge_type,\\\n",
    "                                p=0.2, num_nodes=num_nodes, training=self.training)\n",
    "\n",
    "        out = data.x\n",
    "        h = []\n",
    "        for conv in self.rel_graph_convs:\n",
    "            out = conv(out, edge_index_dr, edge_type_dr)\n",
    "            out = torch.tanh(out)\n",
    "            h.append(out)\n",
    "        h = torch.cat(h, 1)\n",
    "        h = [h[data.x[:, 0] == True], h[data.x[:, 1] == True]]\n",
    "        g = torch.cat(h, 1)\n",
    "        out = self.linear_layer1(g)\n",
    "        out = F.relu(out)\n",
    "        out = F.dropout(out, p=0.5, training=self.training)\n",
    "        out = self.linear_layer2(out)\n",
    "        out = out[:,0]\n",
    "        return out\n",
    "\n",
    "model = IGMC()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_ratings_graph.loc[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/user/Documents/Nauka/Studia/Magisterskie/Magisterka/Recommender/.venv/lib/python3.9/site-packages/torch_geometric/deprecation.py:12: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead\n",
      "  warnings.warn(out)\n"
     ]
    }
   ],
   "source": [
    "LR = 1e-3\n",
    "EPOCHS = 5\n",
    "BATCH_SIZE = 50\n",
    "LR_DECAY_STEP = 20\n",
    "LR_DECAY_VALUE = 10\n",
    "\n",
    "train_loader = DataLoader(train_dataset, BATCH_SIZE, shuffle=True, num_workers=2)\n",
    "test_loader = DataLoader(test_dataset, BATCH_SIZE, shuffle=False, num_workers=2)\n",
    "\n",
    "device = torch.device(\"cpu\")\n",
    "model.to(device)\n",
    "model.reset_parameters()\n",
    "optimizer = Adam(model.parameters(), lr=LR, weight_decay=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1 epoka by trwala 43,5h na CPU\n",
    "loss_through_epochs = []\n",
    "batches_per_epoch = len(train_loader)\n",
    "for epoch in range(1, EPOCHS+1):\n",
    "    model.train()\n",
    "    train_loss_all = 0\n",
    "    for i, train_batch in enumerate(train_loader):\n",
    "        print(f\"{i}/{batches_per_epoch}\")\n",
    "        optimizer.zero_grad()\n",
    "        train_batch = train_batch.to(device)\n",
    "        y_pred = model(train_batch)\n",
    "        y_true = train_batch.y\n",
    "        train_loss = F.mse_loss(y_pred, y_true)\n",
    "        train_loss.backward()\n",
    "        train_loss_all += BATCH_SIZE * float(train_loss)\n",
    "        optimizer.step()\n",
    "        torch.cuda.empty_cache()\n",
    "    train_loss_all = train_loss_all / len(train_loader.dataset)\n",
    "    loss_through_epochs.append(train_loss_all)\n",
    "    print('epoch', epoch,'; train loss', train_loss_all)\n",
    "\n",
    "    if epoch % LR_DECAY_STEP == 0:\n",
    "      for param_group in optimizer.param_groups:\n",
    "          param_group['lr'] = param_group['lr'] / LR_DECAY_VALUE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss from first 5 epochs on 1/100th the dataset and without the feature matrix\n",
      " [1.5614034219933284, 1.2698813516058196, 1.2424809017183027, 1.2205315930388208, 1.2079426253976078]\n"
     ]
    }
   ],
   "source": [
    "a = [1.5614034219933284, 1.2698813516058196, 1.2424809017183027, 1.2205315930388208, 1.2079426253976078]\n",
    "print(\"Loss from first 5 epochs on 1/100th the dataset and without the feature matrix\\n\", a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batches_in_eval = len(test_loader)\n",
    "model.eval()\n",
    "test_loss = 0\n",
    "for i, test_batch in enumerate(test_loader):\n",
    "    print(f\"{i}/{batches_in_eval}\")\n",
    "    test_batch = test_batch.to(device)\n",
    "    with torch.no_grad():\n",
    "        y_pred = model(test_batch)\n",
    "    y_true = test_batch.y\n",
    "    test_loss += F.mse_loss(y_pred, y_true, reduction='sum')\n",
    "    # torch.cuda.empty_cache()\n",
    "mse_loss = float(test_loss) / len(test_loader.dataset)\n",
    "\n",
    "print('test MSE loss', mse_loss)\n",
    "print('test RMSE loss', math.sqrt(mse_loss))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "f24bbe3eee7f1671aa96cb4424e1ae784be864432abf663e2856784d4fd29d28"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
